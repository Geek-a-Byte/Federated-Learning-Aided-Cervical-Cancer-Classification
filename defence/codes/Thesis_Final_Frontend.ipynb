{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sqXpcxlTJHV"
      },
      "outputs": [],
      "source": [
        "#install streamlit, one time execution\n",
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import streamlit\n",
        "import streamlit as st"
      ],
      "metadata": {
        "id": "gx_COAOqY_vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install st-annotated-text"
      ],
      "metadata": {
        "id": "9UM2lZvyhTpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "9FSVHxRHZPhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from googleapiclient.errors import HttpError\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "\n",
        "\n",
        "class GoogleDriveService:\n",
        "    def __init__(self):\n",
        "        self._SCOPES=['https://www.googleapis.com/auth/drive']\n",
        "\n",
        "        _base_path = os.path.dirname('/content/')\n",
        "        _credential_path=os.path.join(_base_path, 'credential.json')\n",
        "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = _credential_path\n",
        "\n",
        "    def build(self):\n",
        "        creds = ServiceAccountCredentials.from_json_keyfile_name(os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"), self._SCOPES)\n",
        "        service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "        return service\n",
        "\n",
        "def create_basic(fid, name, path):\n",
        "    'fid: the folder id of drive under which you would create a new file.'\n",
        "    try:\n",
        "        file_metadata = {'name':name, 'parents': [fid]}\n",
        "        media = MediaFileUpload(path,\n",
        "                                mimetype='application/x-hdf5')\n",
        "        # pylint: disable=maybe-no-member\n",
        "        file = g_drive_service.files().create(body=file_metadata, media_body=media,\n",
        "                                      fields='id').execute()\n",
        "        print(F'File ID: {file.get(\"id\")}')\n",
        "\n",
        "    except HttpError as error:\n",
        "        print(F'An error occurred: {error}')\n",
        "        file = None\n",
        "\n",
        "    return file.get('id')\n",
        "\n",
        "def update_basic(fid, name, path):\n",
        "    'fid: the id of the drive file you want to update'\n",
        "    try:\n",
        "        media = MediaFileUpload(path, mimetype='application/x-hdf5')\n",
        "        # pylint: disable=maybe-no-member\n",
        "        file_metadata = {'name': name}\n",
        "        file = g_drive_service.files().update(fileId=fid,body=file_metadata, media_body=media).execute()\n",
        "        print(F'File ID: {file.get(\"id\")}')\n",
        "\n",
        "    except HttpError as error:\n",
        "        print(F'An error occurred: {error}')\n",
        "        file = None\n",
        "\n",
        "    return file.get('id')\n",
        "\n",
        "selected_fields=\"files(id,name,webViewLink)\"\n",
        "g_drive_service=GoogleDriveService().build()\n",
        "list_file=g_drive_service.files().list(fields=selected_fields).execute()\n",
        "print(list_file)\n",
        "\n",
        "folder_id='1pKC2WaM0UxeBIS_d05kP5TC2wKavuIo9'"
      ],
      "metadata": {
        "id": "EX4nyZYVqh85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten,Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import collections\n",
        "from matplotlib import pyplot as plt\n",
        "class SimpleMLP:\n",
        "    @staticmethod\n",
        "    def build():\n",
        "        model = Sequential() \n",
        "        model.add(Conv2D(filters=16, kernel_size= (3,3), activation= 'relu', input_shape=(66,66,3)) )\n",
        "        model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu' ))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu' ))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "        model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu' ))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "        model.add(Dropout(rate=0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(units=256, activation='relu'))\n",
        "        model.add(Dense(units=128, activation='relu'))\n",
        "        model.add(Dropout(rate=0.25))\n",
        "        model.add(Dense(units=5, activation='sigmoid'))\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        \n",
        "        return model\n",
        "\n"
      ],
      "metadata": {
        "id": "uHsD3GfZqKUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title initialize global model\n",
        "smlp_global = SimpleMLP()\n",
        "global_model = smlp_global.build()\n",
        "global_model.save('global_model.h5')\n",
        "path = \"/content/global_model.h5\"\n",
        "global_update_id=create_basic(folder_id,'global_model.h5', path) # initial copy of global model is created. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el0UKM7dNCtn",
        "outputId": "b7846cca-0db0-40a5-e033-4d72b4f43b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ID: 1oBHRty1v3-ff3QoiF3M433zXmIBWwt07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title initialize local model 1\n",
        "smlp_global = SimpleMLP()\n",
        "local_model = smlp_global.build()\n",
        "local_model.save('local_model_1.h5')\n",
        "path = \"/content/local_model_1.h5\"\n",
        "local1_update_id=create_basic(folder_id,'local_model_1.h5', path) # initial copy of local model is created. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ATGBAMYVOZu",
        "outputId": "f8fdf7d4-32e3-4aaa-b793-69f2bb5b6428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ID: 16Ui-Qmq5FDOBA1lPOhoOqze0ohf-WSIC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title initialize local model 2\n",
        "smlp_global = SimpleMLP()\n",
        "local_model = smlp_global.build()\n",
        "local_model.save('local_model_2.h5')\n",
        "path = \"/content/local_model_2.h5\"\n",
        "local2_update_id=create_basic(folder_id, 'local_model_2.h5', path) # initial copy of local model is created. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXJ3F9i3VO1A",
        "outputId": "263e497b-0dc4-477c-913e-2feb1aeab121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ID: 10BzSLkBET62es9looHQ6C2JXcfc6kOsi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title initialize local model 3\n",
        "smlp_global = SimpleMLP()\n",
        "local_model = smlp_global.build()\n",
        "local_model.save('local_model_3.h5')\n",
        "path = \"/content/local_model_3.h5\"\n",
        "local3_update_id=create_basic(folder_id, 'local_model_3.h5', path) # initial copy of local model is created. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v08ffhV5VOtv",
        "outputId": "b86b0fee-0dc4-4a21-817d-27abcfb6dab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ID: 1XVHi9C3nwaw5Op0sOeOHGVe17qdGs-Fx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "#@title app.py\n",
        "from __future__ import print_function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import keras\n",
        "import keras.utils\n",
        "from keras.layers import Dense,Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import image\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras import utils as np_utils\n",
        "from annotated_text import annotated_text\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "import streamlit as st\n",
        "from googleapiclient.errors import HttpError\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten,Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import collections\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from keras.preprocessing import image\n",
        "import zipfile\n",
        "import cv2\n",
        "import os\n",
        "import shutil \n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import streamlit as st\n",
        "from io import StringIO\n",
        "import shutil\n",
        "import os\n",
        "from PIL import Image \n",
        "\n",
        "###############################################################################################\n",
        "global_update_id = '1oBHRty1v3-ff3QoiF3M433zXmIBWwt07' \n",
        "local1_update_id = '16Ui-Qmq5FDOBA1lPOhoOqze0ohf-WSIC'\n",
        "local2_update_id = '10BzSLkBET62es9looHQ6C2JXcfc6kOsi'\n",
        "local3_update_id = '1XVHi9C3nwaw5Op0sOeOHGVe17qdGs-Fx'\n",
        "locals_upid = [ local1_update_id, local2_update_id, local3_update_id ]\n",
        "\n",
        "lr = 0.001\n",
        "comms_round = 40\n",
        "loss='categorical_crossentropy'\n",
        "optimizer =  tf.keras.optimizers.legacy.SGD(lr=lr, \n",
        "                decay=lr / comms_round, \n",
        "                momentum=0.9\n",
        "               )  \n",
        "metrics = ['accuracy'] # f1_score, precision_score\n",
        "\n",
        "###############################################################################################\n",
        "\n",
        "\n",
        "#@title downloading weights from drive\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "\n",
        "\n",
        "#@title for showing progress of model training\n",
        "class CustomCallback(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Starting training; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Stop training; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        # st.write(\"Epoch \"+str(epoch)+\" starts\")\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        st.write(\"----Epoch----\"+str(epoch)+\" ends\" + \"----Accuracy----\"+str(logs[\"accuracy\"])+\"----Loss----\"+ str(logs[\"loss\"])) \n",
        "        #+ \"----Validation Acc----\"+str(logs[\"val_accuracy\"]) + \"----Validation Loss----\"+str(logs[\"val_loss\"]))\n",
        "        keys = list(logs.keys())\n",
        "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
        "\n",
        "    def on_test_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Start testing; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_test_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_predict_begin(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_predict_end(self, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_test_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_test_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_predict_batch_begin(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n",
        "\n",
        "    def on_predict_batch_end(self, batch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@title broken pipe error solve\n",
        "from requests.exceptions import ConnectionError\n",
        "\n",
        "def retry_on_connectionerror(f,id,updated_by_who, path, max_retries=5):\n",
        "  retries = 0\n",
        "  while retries < max_retries:\n",
        "    try:\n",
        "      return f(id, updated_by_who, path)\n",
        "    except ConnectionError:\n",
        "      retries += 1\n",
        "      st.write(\"retries: \"+retries)\n",
        "  raise Exception(\"Maximum retries exceeded\")\n",
        "\n",
        "##############################################################################\n",
        "\n",
        "class GoogleDriveService:\n",
        "    def __init__(self):\n",
        "        self._SCOPES=['https://www.googleapis.com/auth/drive']\n",
        "\n",
        "        _base_path = os.path.dirname('/content/')\n",
        "        _credential_path=os.path.join(_base_path, 'credential.json')\n",
        "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = _credential_path\n",
        "\n",
        "    def build(self):\n",
        "        creds = ServiceAccountCredentials.from_json_keyfile_name(os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"), self._SCOPES)\n",
        "        service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "        return service\n",
        "\n",
        "def create_basic(fid, name, path):\n",
        "    'fid: the folder id of drive under which you would create a new file.'\n",
        "    try:\n",
        "        file_metadata = {'name':name, 'parents': [fid]}\n",
        "        media = MediaFileUpload(path,\n",
        "                                mimetype='application/x-hdf5')\n",
        "        # pylint: disable=maybe-no-member\n",
        "        file = g_drive_service.files().create(body=file_metadata, media_body=media,\n",
        "                                      fields='id').execute()\n",
        "        print(F'File ID: {file.get(\"id\")}')\n",
        "\n",
        "    except HttpError as error:\n",
        "        print(F'An error occurred: {error}')\n",
        "        file = None\n",
        "\n",
        "    return file.get('id')\n",
        "\n",
        "def update_basic(fid, name, path):\n",
        "    'fid: the id of the drive file you want to update'\n",
        "    try:\n",
        "        media = MediaFileUpload(path, mimetype='application/x-hdf5')\n",
        "        # pylint: disable=maybe-no-member\n",
        "        file_metadata = {'name': name}\n",
        "        file = g_drive_service.files().update(fileId=fid,body=file_metadata, media_body=media).execute()\n",
        "        print(F'File ID: {file.get(\"id\")}')\n",
        "\n",
        "    except HttpError as error:\n",
        "        print(F'An error occurred: {error}')\n",
        "        file = None\n",
        "\n",
        "    return file.get('id')\n",
        "\n",
        "g_drive_service=GoogleDriveService().build()\n",
        "\n",
        "#@title global model initialization\n",
        "\n",
        "# Model architecture: MLP\n",
        "class SimpleMLP:\n",
        "    @staticmethod\n",
        "    def build():\n",
        "        model = Sequential() \n",
        "        model.add(Conv2D(filters=16, kernel_size= (3,3), activation= 'relu', input_shape=(66,66,3)) )\n",
        "        model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu' ))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu' ))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "        model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu' ))\n",
        "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "        model.add(Dropout(rate=0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(units=256, activation='relu'))\n",
        "        model.add(Dense(units=128, activation='relu'))\n",
        "        model.add(Dropout(rate=0.25))\n",
        "        model.add(Dense(units=5, activation='sigmoid'))\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        \n",
        "        return model\n",
        "\n",
        "\n",
        "#@title federated learning main algorithm\n",
        "\n",
        "# Define a function for federated averaging\n",
        "def federated_averaging(client_model, centralized_model):\n",
        "    client_weights = client_model.get_weights()\n",
        "    central_weights = centralized_model.get_weights()\n",
        "    for i in range(len(central_weights)):\n",
        "        central_weights[i] = (central_weights[i] + client_weights[i])/2\n",
        "    centralized_model.set_weights(central_weights)\n",
        "    return centralized_model\n",
        "\n",
        "\n",
        "#@title training clients\n",
        "\n",
        "def train_local_models(client, global_model, client_drive_id):\n",
        "    # Adding Model check point Callback\n",
        "    mc = ModelCheckpoint(filepath=\"/content/cervical_cancer_best_model\"+\"_updated_by_\"+client+\".h5\", save_freq=\"epoch\", monitor= 'val_accuracy', verbose= 1, save_best_only= True, mode = 'auto');\n",
        "    call_back = [ mc ];\n",
        "    global_weights = global_model.get_weights()\n",
        "    #loop through each client and create new local model\n",
        "    # for client in client_names:\n",
        "    st.write(\"Client_\"+client+\" is training now\")\n",
        "    \n",
        "    # smlp_local = SimpleMLP()\n",
        "    # local_model = smlp_local.build()\n",
        "    # local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "    file_id = client_drive_id\n",
        "    path='local_model_received_by_'+client+'.h5'\n",
        "    download_file_from_google_drive(file_id, path)\n",
        "    local_model = load_model(path)\n",
        "    \n",
        "    #set local model weight to the weight of the global model\n",
        "    # local_model.set_weights(global_weights)\n",
        "    'ekhane jokhn ekmatro tokhoni global model weight set korbo'\n",
        "    local_model.fit(client_train_batched[client],steps_per_epoch=3, epochs=50, validation_data= client_val_batched[client], \n",
        "                  validation_steps= 6, verbose=2, callbacks=call_back)\n",
        "    local_model = load_model(\"/content/cervical_cancer_best_model\"+\"_updated_by_\"+client+\".h5\")\n",
        "\n",
        "    return local_model\n",
        "\n",
        "\n",
        "## frontend code starts\n",
        "\n",
        "st.markdown(\n",
        "         f\"\"\"\n",
        "         <style>\n",
        "         [data-testid=\"stSidebar\"] > div:first-child {{\n",
        "             background: url(\"https://miro.medium.com/max/1024/1*4nKg-FASHe3XW50hHexYDQ.png\");\n",
        "             background-attachment: fixed;\n",
        "         }}\n",
        "         </style>\n",
        "         \"\"\",\n",
        "         unsafe_allow_html=True\n",
        "     )\n",
        "\n",
        "with st.sidebar:\n",
        "    original_title = '<p style=\"color:Black; font-size: 35px; font-weight: bold\">Federated Learning Aided Cervical Cancer Classification WebApp</p>'\n",
        "    st.markdown(original_title, unsafe_allow_html=True)\n",
        "    page = st.sidebar.selectbox( 'Select User Type',['Client','Admin Aggregator']) \n",
        "\n",
        "if page=='Client':\n",
        "    #############################################################################################\n",
        "    \n",
        "    st.subheader(\"Step 0\")  \n",
        "    annotated_text(\n",
        "        (\"Enter the ID of your hospital: i.e. 1,2 or 3\", \"Step 0\"),\n",
        "    )\n",
        "    #adding a single-line text input widget\n",
        "    client_name = st.text_input(\"\",'0')\n",
        "    #displaying the entered text\n",
        "    st.write('Thank you for your input, the ID number of this client device is', int(client_name)) \n",
        "    st.markdown(\"---------------------------------\")\n",
        "\n",
        "    #############################################################################################\n",
        "\n",
        "    #############################################################################################\n",
        "\n",
        "    st.subheader(\"Step 1\")  \n",
        "    annotated_text(\n",
        "        (\"Request for currently updated Global Model\", \"Step 1\"),\n",
        "    )\n",
        "    st.markdown(\"\")  \n",
        "    global_model_requested = st.button('Request Global Model From Server')\n",
        "    if global_model_requested:\n",
        "      file_id = global_update_id\n",
        "      destination = '/content/global_model.h5'\n",
        "      download_file_from_google_drive(file_id, destination)\n",
        "      global_model = load_model(destination)\n",
        "      st.write(\"Global model received from server in the following path\", destination)\n",
        "    st.markdown(\"---------------------------------\")  \n",
        "\n",
        "    #############################################################################################\n",
        "\n",
        "    #############################################################################################\n",
        "\n",
        "    st.subheader(\"Step 2\") \n",
        "    annotated_text(\n",
        "        (\"Please upload a zip file of your train images:\", \"Step 2\"),\n",
        "    )\n",
        "    st.markdown(\"\")  \n",
        "\n",
        "    if(os.path.isdir('/content/content/Client_'+client_name)==True):\n",
        "          shutil.rmtree('/content/content/Client_'+client_name)\n",
        "    uploaded_file = st.file_uploader(\"Choose a file\",type=\"zip\")\n",
        "    \n",
        "    if uploaded_file is not None:\n",
        "      with zipfile.ZipFile(uploaded_file, \"r\") as z:\n",
        "        z.extractall(\".\")\n",
        "        st.write(\"Hurray! Your dataset has been successfully uploaded.\")\n",
        "    st.markdown(\"---------------------------------\") \n",
        "\n",
        "    #############################################################################################\n",
        "\n",
        "    #############################################################################################\n",
        "\n",
        "    st.subheader(\"Step 3\") \n",
        "    annotated_text(\n",
        "        (\"Train your local model and save as .h5 format locally\", \"Step 3\"),\n",
        "    )\n",
        "    st.markdown(\"\") \n",
        "    train = st.button('Train')\n",
        "    if train:\n",
        "        st.write('Client '+client_name+' started training now.')\n",
        "       \n",
        "        # process and batch the training data for each client\n",
        "        # client_name format: clients_1\n",
        "        train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(zoom_range = 0.2, shear_range = 0.2 , rescale = 1./255 , horizontal_flip=True)\n",
        "        train_data = train_datagen.flow_from_directory(directory= \"/content/Client_\"+client_name, target_size=(66, 66), batch_size=10, class_mode = 'categorical')\n",
        "        \n",
        "        # fetch the global model for this round for one client\n",
        "        global_model = load_model('/content/global_model.h5')\n",
        "        global_weights = global_model.get_weights()\n",
        "       \n",
        "        # set weights of local model to global model's weights and \n",
        "        smlp_local = SimpleMLP()\n",
        "        local_model = smlp_local.build()\n",
        "        local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "        \n",
        "        #set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "\n",
        "        local_model.fit(train_data, steps_per_epoch=100, epochs=10, verbose=1, callbacks=[CustomCallback()])\n",
        "        \n",
        "        local_model_name = 'local_model_'+ client_name + '.h5'\n",
        "        path = \"/content/\"+ local_model_name\n",
        "        local_model.save(path)\n",
        "        \n",
        "        \n",
        "    st.markdown(\"---------------------------------\") \n",
        "    \n",
        "    #############################################################################################\n",
        "\n",
        "    #############################################################################################\n",
        "\n",
        "    st.subheader(\"Step 4\") \n",
        "    annotated_text(\n",
        "        (\"Send your locally trained parameters to the server, your data would be safe with yourself.\", \"Step 4\"),\n",
        "    )\n",
        "    st.markdown(\"\") \n",
        "    send = st.button('Send')\n",
        "    \n",
        "    if send:\n",
        "      local_model_name = 'local_model_'+ client_name + '.h5'\n",
        "      path = \"/content/\"+ local_model_name\n",
        "      id = update_basic(locals_upid[int(client_name)-1], local_model_name, path)\n",
        "      st.write(\"Successfully sent the parameters to the server.\")\n",
        "   \n",
        "    \n",
        "    \n",
        "    st.markdown(\"---------------------------------\") \n",
        "    \n",
        "    #############################################################################################\n",
        "\n",
        "    #############################################################################################\n",
        "\n",
        "    st.subheader(\"Step 5\") \n",
        "    annotated_text(\n",
        "        (\"Upload a test image and see the output of the class and accuracy.\", \"Step 5\"),\n",
        "    )\n",
        "    st.markdown(\"\") \n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Choose a test image file\",type=\"zip\")\n",
        "    \n",
        "    if uploaded_file is not None:\n",
        "      with zipfile.ZipFile(uploaded_file, \"r\") as z:\n",
        "        z.extractall(\".\")\n",
        "        st.write(\"Hurray! Your test dataset has been successfully uploaded.\")\n",
        "\n",
        "    test = st.button('Test')\n",
        "    if test:      \n",
        "        local_model_name = 'local_model_'+ client_name + '.h5'\n",
        "        path = \"/content/\"+ local_model_name\n",
        "        local_model = load_model(path)\n",
        "        \n",
        "        test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
        "        test_data = test_datagen.flow_from_directory(directory= \"/content/test/\", target_size=(66, 66), batch_size=10, class_mode = 'categorical')\n",
        "\n",
        "        accuracy1 = local_model.evaluate_generator(generator= test_data)[1] \n",
        "\n",
        "        file_id = global_update_id\n",
        "        destination = '/content/global_model.h5'\n",
        "        download_file_from_google_drive(file_id, destination)\n",
        "        global_model = load_model(destination)\n",
        "\n",
        "        accuracy2 = global_model.evaluate_generator(generator= test_data)[1] \n",
        "\n",
        "        st.write(\"The accuracy of your local model is = \"+ str(accuracy1*100) +\" %\")\n",
        "        st.write(\"The accuracy of the global model is = \"+ str(accuracy2*100) +\" %\")\n",
        "\n",
        "    st.markdown(\"---------------------------------\") \n",
        "\n",
        "if page=='Admin Aggregator':\n",
        "    \n",
        "    st.subheader(\"Step 0\")  \n",
        "    annotated_text(\n",
        "        (\"Enter the admin credential ID\", \"Step 0\"),\n",
        "    )\n",
        "    #adding a single-line text input widget\n",
        "    admin_name = st.text_input(\"\",'0')\n",
        "    #displaying the entered text\n",
        "    if(admin_name==\"admin\"):\n",
        "      st.write('Admin Verified')\n",
        "      st.subheader(\"Step 1\") \n",
        "      annotated_text(\n",
        "          (\"Please aggregate the local updates with the global model\", \"Step 1\"),\n",
        "      )\n",
        "      st.markdown(\"\") \n",
        "      aggre = st.button('Aggregate') \n",
        "      if aggre:\n",
        "        file_id = local1_update_id\n",
        "        destination = '/content/local_model_1.h5'\n",
        "        download_file_from_google_drive(file_id, destination)\n",
        "        local1 = load_model(destination)\n",
        "        st.write(\"Local model received from server in the following path\", destination)\n",
        "\n",
        "        file_id = local2_update_id\n",
        "        destination = '/content/local_model_2.h5'\n",
        "        download_file_from_google_drive(file_id, destination)\n",
        "        local2 = load_model(destination)\n",
        "        st.write(\"Local model received from server in the following path\", destination)\n",
        "\n",
        "        file_id = local3_update_id\n",
        "        destination = '/content/local_model_3.h5'\n",
        "        download_file_from_google_drive(file_id, destination)\n",
        "        local3 = load_model(destination)\n",
        "        st.write(\"Local model received from server in the following path\", destination)\n",
        "\n",
        "        \n",
        "        locals = [local1, local2, local3]\n",
        "        \n",
        "        \n",
        "        # fetch the global model for this round for one client\n",
        "        global_model = load_model('/content/global_model.h5')\n",
        "        global_weights = global_model.get_weights()\n",
        "       \n",
        "        # set weights of local model to global model's weights and \n",
        "        smlp_local = SimpleMLP()\n",
        "        temp_model = smlp_local.build()\n",
        "        temp_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "        \n",
        "        #set local model weight to the weight of the global model\n",
        "        temp_model.set_weights(global_weights)\n",
        "\n",
        "        temp_model = federated_averaging(local1,temp_model)\n",
        "        temp_model = federated_averaging(local2,temp_model)\n",
        "        temp_model = federated_averaging(local3,temp_model)\n",
        "\n",
        "        path = '/content/global_model.h5'\n",
        "        temp_model.save(path)\n",
        "        id = update_basic(global_update_id, 'global_model.h5', path)\n",
        "        st.write(\"Successfully aggregated the parameters to the global model.\")\n",
        "         \n",
        "   \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5sG6rrFpuDi",
        "outputId": "33120c66-be8b-4269-9c4b-7d0a01211c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "8i3G2CS5SDey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(\"2GJ3XbpGRoKhs9VWiuCBUxlPuRM_7veAjgLij9ne349wXFQjg\") #ngrok.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSRk_L1-qpzZ",
        "outputId": "df0e09ef-c98c-42db-e45f-ae966debccfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process:Updating authtoken for default \"config_path\" of \"ngrok_path\": /usr/local/lib/python3.8/dist-packages/pyngrok/bin/ngrok\n",
            "2023-02-25 21:23:39.581 INFO    pyngrok.process: Updating authtoken for default \"config_path\" of \"ngrok_path\": /usr/local/lib/python3.8/dist-packages/pyngrok/bin/ngrok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run app.py --server.port 80 &\n",
        "url = ngrok.connect(port = '80')\n",
        "print(url)"
      ],
      "metadata": {
        "id": "fzl3qHqjnSxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tunnels = ngrok.get_tunnels()\n",
        "tunnels\n",
        "\n",
        "# try:\n",
        "#     # Block until CTRL-C or some other terminating event\n",
        "#     ngrok_process.proc.wait()\n",
        "# except KeyboardInterrupt:\n",
        "#     print(\" Shutting down server.\")\n",
        "\n",
        "#     ngrok.kill()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PW-rQEMnWNw",
        "outputId": "3fff70f1-841a-4807-af11-7ecaacea24fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process.ngrok:t=2023-02-24T16:15:49+0000 lvl=info msg=start pg=/api/tunnels id=1dbb1098a4b06a4e\n",
            "2023-02-24 16:15:49.907 INFO    pyngrok.process.ngrok: t=2023-02-24T16:15:49+0000 lvl=info msg=start pg=/api/tunnels id=1dbb1098a4b06a4e\n",
            "INFO:pyngrok.process.ngrok:t=2023-02-24T16:15:49+0000 lvl=info msg=end pg=/api/tunnels id=1dbb1098a4b06a4e status=200 dur=253.622µs\n",
            "2023-02-24 16:15:49.911 INFO    pyngrok.process.ngrok: t=2023-02-24T16:15:49+0000 lvl=info msg=end pg=/api/tunnels id=1dbb1098a4b06a4e status=200 dur=253.622µs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<NgrokTunnel: \"https://0af6-34-73-195-65.ngrok.io\" -> \"http://localhost:80\">,\n",
              " <NgrokTunnel: \"http://0af6-34-73-195-65.ngrok.io\" -> \"http://localhost:80\">]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woLoVdKanX9S",
        "outputId": "f28617f5-a0f4-475b-ed96-9a88ab449215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process:Killing ngrok process: 1568\n",
            "2023-02-24 16:16:01.919 INFO    pyngrok.process: Killing ngrok process: 1568\n"
          ]
        }
      ]
    }
  ]
}