{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQwNOhT9LuaeZ8SR7V9J6A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Geek-a-Byte/thesis/blob/main/Step_4_Train_local_model_(wrong).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_32lbSvKLqgN"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "TaMO67r5LuPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# Import required modules\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from pprint import pprint\n",
        "\n",
        "import streamlit as st\n",
        "from io import StringIO\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K\n",
        "import collections\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "scope = [\"https://spreadsheets.google.com/feeds\", 'https://www.googleapis.com/auth/spreadsheets',\n",
        "\t\t\"https://www.googleapis.com/auth/drive.file\", \"https://www.googleapis.com/auth/drive\"]\n",
        "\n",
        "\n",
        "\n",
        "# Assign credentials ann path of style sheet\n",
        "creds = ServiceAccountCredentials.from_json_keyfile_name(\"/content/thesis-367714-64137e9747ad.json\", scope)\n",
        "client = gspread.authorize(creds)\n",
        "spreadsheet = client.open(\"thesis_aliya\")\n",
        "sheet = spreadsheet.sheet1\n",
        "\n",
        "def load(paths, verbose=-1):\n",
        "    '''expects images for each class in seperate dir'''\n",
        "    data = list()\n",
        "    labels = list()\n",
        "    # loop over the input images\n",
        "    for (i, imgpath) in enumerate(paths):\n",
        "        # load the image and extract the class labels\n",
        "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
        "        image = np.array(im_gray).flatten()\n",
        "        label = imgpath.split(os.path.sep)[-2] # Metaplastic, Parabasal, Koilocytotic, Superficial-Intermediate, Dyskeratotic\n",
        "        # scale the image to [0, 1] and add to list\n",
        "        data.append(image/255)\n",
        "        labels.append(label)\n",
        "        # show an update every `verbose` images\n",
        "        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
        "            print(label);\n",
        "            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n",
        "    # return a tuple of the data and labels\n",
        "    return data, labels\n",
        "\n",
        "class SimpleMLP:\n",
        "    @staticmethod\n",
        "    def build(shape, classes):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(200, input_shape=(shape,)))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(200))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(classes))\n",
        "        model.add(Activation(\"softmax\"))\n",
        "        return model\n",
        "\n",
        "#process and batch the training data for each client\n",
        "clients_batched = dict()\n",
        "\n",
        "comms_round = 100\n",
        "\n",
        "#create optimizer\n",
        "lr = 0.01 \n",
        "loss='categorical_crossentropy'\n",
        "metrics = ['accuracy'] # f1_score, precision_score\n",
        "optimizer = SGD(lr=lr, \n",
        "                decay=lr / comms_round, \n",
        "                momentum=0.9\n",
        "               ) \n",
        "\n",
        "\n",
        "# Function for preparing client data for training\n",
        "def batch_data(data_shard, bs=32):\n",
        "    '''Takes in a clients data shard and create a tfds object off it\n",
        "    args:\n",
        "        shard: a data, label constituting a client's data shard\n",
        "        bs:batch size\n",
        "    return:\n",
        "        tfds object'''\n",
        "    #seperate shard into data and labels lists\n",
        "    data, label = zip(*data_shard)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
        "    return dataset.shuffle(len(label)).batch(bs)\n",
        "\n",
        "def weight_scalling_factor(clients_trn_data, client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "    #get the bs\n",
        "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
        "    #first calculate the total training data points across clinets\n",
        "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
        "    return local_count/global_count\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "\n",
        "def train_single_model(global_model,client,image_list,label_list):\n",
        "    data = list(zip(image_list, label_list))\n",
        "    random.shuffle(data)\n",
        "    \n",
        "    smlp_local = SimpleMLP()\n",
        "    local_model = smlp_local.build(4356, 5)\n",
        "    local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "    \n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "    \n",
        "    # set local model weight to the weight of the global model\n",
        "    local_model.set_weights(global_weights)\n",
        "    \n",
        "    # batch the data\n",
        "    clients_batched[client] = batch_data(data)\n",
        "    \n",
        "    #fit local model with client's data\n",
        "    tf.autograph.experimental.do_not_convert(local_model.fit(clients_batched[client], epochs=1, verbose=0))\n",
        "    \n",
        "    steps = len(local_model.get_weights())\n",
        "    st.write(steps)\n",
        "\n",
        "    #scale the model weights and add to list\n",
        "    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
        "    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "    \n",
        "    for i in range(0,len(scaled_weights)):\n",
        "      arr = scaled_weights[i]\n",
        "      lis = arr.tolist()\n",
        "      j=0\n",
        "      for x in lis:\n",
        "        j+=1\n",
        "        sheet.insert_row(x, j)\n",
        "    \n",
        "    #clear session to free memory after each communication round\n",
        "    K.clear_session()\n",
        "\n",
        "    return scaled_weights\n",
        "    \n",
        "import zipfile\n",
        "\n",
        "app_mode = st.sidebar.selectbox('Select Page',['Home','About'])\n",
        "if app_mode=='Home': \n",
        "    st.title('Classify Cervical Cancer') \n",
        "    st.markdown('upload photographs in bulk :')\n",
        "    #adding a single-line text input widget\n",
        "    name = st.text_input('Enter the id of your institution: ', '0')\n",
        "    #displaying the entered text\n",
        "    st.write('the name is ', name) \n",
        "    if(os.path.isdir('/content/content/Client_'+name)==True):\n",
        "          shutil.rmtree('/content/content/Client_'+name)\n",
        "    uploaded_file = st.file_uploader(\"Choose a file\",type=\"zip\")\n",
        "    if uploaded_file is not None:\n",
        "      with zipfile.ZipFile(uploaded_file, \"r\") as z:\n",
        "        z.extractall(\".\")\n",
        "   \n",
        "      # To convert to a string based IO:\n",
        "      # stringio = StringIO(uploaded_file.getvalue().decode(\"utf-8\"))\n",
        "\n",
        "      # To read file as string:\n",
        "      # string_data = stringio.read()\n",
        "\n",
        "    \n",
        "    #adding a button\n",
        "    \n",
        "    col1, col2, col3 = st.columns([1,1,1])\n",
        "\n",
        "    with col1:\n",
        "        train = st.button('Train')\n",
        "    with col2:\n",
        "        test = st.button('Test')\n",
        "    with col3:\n",
        "        aggregate = st.button('Aggregate')\n",
        "\n",
        "    if test:\n",
        "       st.write('testing') #displayed when the button is clicked\n",
        "    \n",
        "    if train:\n",
        "      \n",
        "       \n",
        "       #declear path to your data folder\n",
        "       img_path = '/content/content/Client_'+name\n",
        "\n",
        "\n",
        "       #get the path list using the path object\n",
        "       image_paths = list(paths.list_images(img_path))\n",
        "\n",
        "       #apply our function\n",
        "       image_list, label_list = load(image_paths, verbose=10000)\n",
        "           \n",
        "       #binarize the labels\n",
        "       lb = LabelBinarizer()\n",
        "       label_list = lb.fit_transform(label_list)\n",
        "\n",
        "       #split data into training and test set\n",
        "       X_train, X_test, y_train, y_test = train_test_split(image_list, \n",
        "                                                    label_list, \n",
        "                                                    test_size=0.1, \n",
        "                                                    random_state=42)\n",
        "       smlp_global = SimpleMLP()\n",
        "       global_model = smlp_global.build(4356, 5)\n",
        "       train_single_model(global_model,'aliya',image_list,label_list)\n",
        "       st.write('training') #displayed when the button is clicked\n",
        "    \n",
        "    if aggregate:\n",
        "       st.write('aggregating') #displayed when the button is clicked\n",
        "\n",
        "if app_mode=='About': \n",
        "      st.title('Classify Cervical Cancer') \n",
        " \n",
        "\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcTd3FViLvi9",
        "outputId": "b13ace3e-d857-4153-8ac0-29ab09ff736f"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "P3NZWg18LxUI"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6nt_FbNLyh5",
        "outputId": "663d9bd3-80de-4429-b03f-edf0edae4c30"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.503s\n",
            "your url is: https://angry-shrimps-move-35-234-33-190.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from pprint import pprint\n",
        "\n",
        "scope = [\"https://spreadsheets.google.com/feeds\", 'https://www.googleapis.com/auth/spreadsheets',\n",
        "\t\t\"https://www.googleapis.com/auth/drive.file\", \"https://www.googleapis.com/auth/drive\"]\n",
        "\n",
        "\n",
        "\n",
        "# Assign credentials ann path of style sheet\n",
        "creds = ServiceAccountCredentials.from_json_keyfile_name(\"/content/thesis-367714-64137e9747ad.json\", scope)\n",
        "client = gspread.authorize(creds)\n",
        "sheet = client.open(\"thesis_aliya\").sheet1\n",
        "\n",
        "\n",
        "\n",
        "# display data\n",
        "data = sheet.get_all_records()\n",
        "row4 = sheet.row_values(4)\n",
        "col2 = sheet.col_values(2)\n",
        "cell = sheet.cell(5, 2).value\n",
        "\n",
        "print(\"Column 2 Data : \")\n",
        "pprint(col2)\n",
        "print(\"\\nRow 4 Data : \")\n",
        "pprint(row4)\n",
        "print(\"\\nCell (5,2) Data : \")\n",
        "pprint(cell)\n",
        "print(\"\\nAll Records : \")\n",
        "pprint(data)\n",
        "\n",
        "\n",
        "\n",
        "# Inserting data\n",
        "insertRow = [6, \"Soumodeep Naskar\", \"Purple\"]\n",
        "sheet.insert_row(insertRow, 4)\n",
        "print(\"\\nAll Records after inserting new row : \")\n",
        "pprint(data)\n",
        "\n",
        "\n",
        "\n",
        "# Deleting data\n",
        "sheet.delete_row(7)\n",
        "print(\"\\nAll Records after deleting row 7 : \")\n",
        "pprint(data)\n",
        "\n",
        "\n",
        "\n",
        "# Update a cell\n",
        "sheet.update_cell(5, 2, \"Nitin Das\")\n",
        "print(\"\\nAll Records after updating cell (5,2) : \")\n",
        "pprint(data)\n",
        "\n",
        "\n",
        "\n",
        "# Display no. of rows, columns\n",
        "# and no. of rows having content\n",
        "numRows = sheet.row_count\n",
        "numCol = sheet.col_count\n",
        "print(\"Number of Rows : \", numRows)\n",
        "print(\"Number of Columns : \", numCol)\n",
        "print(\"Number of Rows having content : \", len(data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQxMmSYbPRJz",
        "outputId": "f5c21638-66cc-4a42-9be5-e73b6881af9d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 2 Data : \n",
            "[]\n",
            "\n",
            "Row 4 Data : \n",
            "[]\n",
            "\n",
            "Cell (5,2) Data : \n",
            "''\n",
            "\n",
            "All Records : \n",
            "[]\n",
            "\n",
            "All Records after inserting new row : \n",
            "[]\n",
            "\n",
            "All Records after deleting row 7 : \n",
            "[]\n",
            "\n",
            "All Records after updating cell (5,2) : \n",
            "[]\n",
            "Number of Rows :  1000\n",
            "Number of Columns :  26\n",
            "Number of Rows having content :  0\n"
          ]
        }
      ]
    }
  ]
}