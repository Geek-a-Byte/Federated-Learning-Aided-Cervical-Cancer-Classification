{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FL practise",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0JEVLOOJZtTVrQ4Y7aOdw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Geek-a-Byte/thesis/blob/main/FL_practise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/trainingSet.zip'"
      ],
      "metadata": {
        "id": "nGybxfERgWs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.2.0\n",
        "!sudo pip3 install keras"
      ],
      "metadata": {
        "id": "w09jlAEvHRxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuqExgKuFtUa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Reading and preprocessing MNIST data set**\n",
        "\n",
        "On line 9, each image will be read from disk as grey scale and then flattened. The flattening step is import because we will be using a MLP network architecture later on. To obtain the class label of an image, we split its path string on line 11. Hope you noticed we also scaled the image to [0, 1] on line 13 to douse the impact of varying pixel brightness."
      ],
      "metadata": {
        "id": "GehWpwa0Hoja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load(paths, verbose=-1):\n",
        "    '''expects images for each class in seperate dir, \n",
        "    e.g all digits in 0 class in the directory named 0 '''\n",
        "    data = list()\n",
        "    labels = list()\n",
        "    # loop over the input images\n",
        "    for (i, imgpath) in enumerate(paths):\n",
        "        # load the image and extract the class labels\n",
        "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
        "        image = np.array(im_gray).flatten()\n",
        "        label = imgpath.split(os.path.sep)[-2] # 0 1 2 3 4 5 6 7 8 9\n",
        "        # scale the image to [0, 1] and add to list\n",
        "        data.append(image/255)\n",
        "        labels.append(label)\n",
        "        # show an update every `verbose` images\n",
        "        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
        "            print(label);\n",
        "            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n",
        "    # return a tuple of the data and labels\n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "1usnYxYbGI5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Federated Members (clients) as Data Shards**\n",
        "\n",
        "In the real world implementation of FL, each federated member will have its own data coupled with it in isolation. Remember the aim of FL is to ship models to data and not the other way around. The shard creation step here only happens in experiments. I will share the training set into 10 shards, one per client. I wrote a function called create_clients to achieve this.\n",
        "\n",
        "On line 13, I created a list of client names using the prefix (initials). On line 16–20, I zipped the data and label lists then randomised the resulting tuple list. Finally I created shards from the tuple list based on the desired number of clients (num_clients) on line 21. On line 26, a dictionary containing each client’s name as key and their data share as value was returned. Let’s now go ahead and apply this function to our training data set."
      ],
      "metadata": {
        "id": "nuDutNyXZNp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n",
        "    ''' return: a dictionary with keys clients' names and value as \n",
        "                data shards - tuple of images and label lists.\n",
        "        args: \n",
        "            image_list: a list of numpy arrays of training images\n",
        "            label_list:a list of binarized labels for each image\n",
        "            num_client: number of fedrated members (clients)\n",
        "            initials: the clients'name prefix, e.g, clients_1 \n",
        "            \n",
        "    '''\n",
        "\n",
        "    #create a list of client names\n",
        "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
        "\n",
        "    #randomize the data\n",
        "    data = list(zip(image_list, label_list))\n",
        "    random.shuffle(data)\n",
        "\n",
        "    #shard data and place at each client\n",
        "    size = len(data)//num_clients\n",
        "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
        "\n",
        "    #number of clients must equal number of shards\n",
        "    assert(len(shards) == len(client_names))\n",
        "\n",
        "    return {client_names[i] : shards[i] for i in range(len(client_names))}"
      ],
      "metadata": {
        "id": "nwIX2OcJZMLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Processing and batching clients’ and test data**\n",
        "\n",
        "Next is to process each of the client’s data into tensorflow data set and batch them. To simplify this step and avoid repetition, I encapsulated the procedure into a small function named batch_data.\n",
        "\n",
        "I trust you remember that each of the client data sets came out as a (data, label) tuple list from create_clients. On line 9 above, I split the tuple into separate data and label lists. I then made a shuffled and batched tensorflow dataset object off these lists."
      ],
      "metadata": {
        "id": "fCp7sXAyikj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for preparing client data for training\n",
        "def batch_data(data_shard, bs=32):\n",
        "    '''Takes in a clients data shard and create a tfds object off it\n",
        "    args:\n",
        "        shard: a data, label constituting a client's data shard\n",
        "        bs:batch size\n",
        "    return:\n",
        "        tfds object'''\n",
        "    #seperate shard into data and labels lists\n",
        "    data, label = zip(*data_shard)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
        "    return dataset.shuffle(len(label)).batch(bs)"
      ],
      "metadata": {
        "id": "W8vqTJqkd7vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating the Multi Layer Perceptron (MLP) model**\n",
        "\n",
        "One thing I didn't mention in the introduction section is that FL is mostly suited for parameterized learning — all types of neural networks. Machine learning techniques such as KNN or it likes that merely store training data while learning might not benefit from FL. I’m creating a 3-layer MLP to serve as the model for our classification task. I hope you still remember all those Keras modules we imported earlier, this is where they fit in."
      ],
      "metadata": {
        "id": "XcQohmOLkkJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model architecture: MLP\n",
        "class SimpleMLP:\n",
        "    @staticmethod\n",
        "    def build(shape, classes):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(200, input_shape=(shape,)))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(200))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(classes))\n",
        "        model.add(Activation(\"softmax\"))\n",
        "        return model"
      ],
      "metadata": {
        "id": "7XbrvXQyfzCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Aggregation (Federated Averaging)**\n",
        "All I have done up to this point was pretty much standard as per deep learning pipeline. Of course with the exception of the data partitioning or client creation bit. I will now move on to Federated Averaging ( the vanilla algorithm for FL) which is the whole point of the this tutorial. The data I’m using is horizontally partitioned, so I will simply be doing component wise parameter averaging which will be weighed based on the proportion of data points contributed by each participating client. Here’s the federated averaging equation I’m using, it comes one of the pioneering works on federated learning [2].\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAr8AAACKCAYAAACjMSLZAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAAGQcSURBVHhe7d0HYNxnffj/9+mkm7qT7qTT3pI1bHnv7TjD2ZABIUDY0EIp/Zdf6a+l0PZHW1qgBVpooRAIJGFk7ziJHe+9tay9t3QnnU635//5npXEdpTEjmUnjp4XvWJ0p++dvvd8P8/neb7PUMUEJEmSJEmSJGkWSJj6b0mSJEmSJEn6wJPJryRJkiRJkjRryORXkiRJkiRJmjVk8itJkiRJkiTNGjL5lSRJkiRJkmYNmfxKkiRJkiRJs4ZMfiVJkiRJkqRZQya/kiRJkiRJ0qwhk19JkiRJkiRp1pDJryRJkiRJkjRryORXkiRJkiRJmjVk8itJkiRJkiTNGjL5lSRJkiRJkmYNmfxKkiRJkiRJs4ZMfiVJkiRJkqRZQya/kiRJkiRJ0qwhk19JkiRJkiRp1pDJryRJkiRJkjRrqGLC1L8l6SIF8Yz209/cQHNHCwfrIb00n5KqORTkFTK/xEKieJUqOEpP7REO7jnJfncGS+dXMX9RJcW56Vg0qjOHkiRJkiRJugJkz690SWLRMAHXIP3HnuGhP+5l/+lB7IEw4chrbaoQ3tEOOhrrONbQzZDLjy8QEs9HiU69QpIkSZIk6UqRPb/SJfIz2XuC47/4Jn+ycz33fu2jfPaOagoTY8Qi4rnxUQabT9LYH2DMWMWWzdVk6BJIks0uSZIkSZLeAzIFkS5NzIfPNUTrsQ4SqwvJKbSSnqQiFvYScjSw75Gn2T+UinX5Fj51y3xyDTLxlSRJkiTpvSPTEOnS+F14hgaprVNTlJ1FXqoW1Xg77cd3cv8j3SQtu4EN6xaxONeEWiXH90qSJEmS9N6Sya90SSKeCcYHRzgxVkBZnpGEgSNsf+gh/vvnr3A8loEtP5dcWwomrRqZ+kqSJEmS9F6Tya90CSIEJh2MDvTTEtLiGelnxD5K/9AgAx1t1HS7mAzFiMq0V5IkSZKk9wmZ/EqXIIh7fISB/h5cFjX2vmECqRVUX7eBW1ab6X3xZY62jzLsDSNnVUqSJEmS9H4gk1/p3Yu5GB/spa/PQ+ptn+CLX/sMH920kvUrV7P2mlXc4niJ/YeaaR10y2XNJEmSJEl6X5DJr/Tu+ccZ6x1juF/H3MXzKLLoSU5UgT6DtPIl3PXpAlr3HeVIXS+9ftn3K0mSJEnSe08mv9K7FCPsGKB3YJJ2Xw5LK7OxGDSoleG9CQaSbUUsvXUzC5w1dNU2UNM9IXt/JUmS3kOx0CST/bUceuRnPHa4g1ZHcOoZSZpdZPIrXZxYWPzfOAMtp6k7epyTrUN0+dXoXP0MOX24g0qKG42P8Y0kmckMttB2ZDfbth3ieGMfjpBImmUnsCRJ0hUQETHbSe/JQxx8aSsvP/8szz/3NA/84hFeqe1nYDI09TpJml1k8itdpIhIfl0Md7bQ2uPCb0ilqNKGbqyH0Qkv3pCS/IYJhQM47FGsFQWkajxMdLXR0DbEuMh8X9/5WJIkSbqMlOTXxWDDCU7s3sWeHa+yY/d+ntrRx6gzQFRu8CrNUnJ7Y0mSJEmaBaL2Wupf/j133fc4C773c7569yquKU6eelaSZg/Z8ytJkiRJkiTNGjL5lSRJkiRJkmYNmfxKV7kYUXcfHY2nOLBjJ3u272BnyxhjE6MMtNZy6tAB9h06xIn9L/HM1sMcax1hPCDXnZAkSZKk2Uomv9JVTBmuHmSitYb6Uyc4sPdVXn3qd/zXY/vYf6qehoZa6utqqa2ro6l2J0//4VGe2VVD3bBXmQYiSZIkSdIsJJNf6SoWgugItdu6SEqfx8Y7NrK8wMvTP/wBP/5dPYNpK7n2U1/iK1/8FB//0y/yEVs9bacO8OLJAfxTRzgjRjQSJhTw4/P6CYajROU0UEmSJEn6QJLJr3T1igSIudo5nVSC1pSCJTSGfWgCrIu57vbNrKwuwKZRirjIZGMRIsEQEy4fTk9ASZtfF/P10Lr3d/zXn3+JDy/9LP/2ZB0Ndrn4uyRJkiR9EMmlzqQpAaLBfg7+7Je8fHqADs+VHhiQSdUNt3PNNctZnW9A2SjuHUW8xDyt7K43kZ8XJVD3DE/84nF+mPxVHv77G1lfZsWsbDkXdhFzHuLHn/5HXrXdzLJ7P8nfbilCO3UYwpOMtR1k31PP83ffdfGp3/0NH9o0h3KzeuoFkiRJV78rvtRZdIjG559h56sHOeAIT/3wSkklq2ola+68ndsqzCQlXFCtIs0S6n8Upv4tzWoRYhEX3Tse46nntvPCzkOcqG+locWNJreArMx0bBYzpuRkki/yYdQlkqiKEgqGiIS8uCfGsA920d5wilMHD3Gkpo66ulb61NlYC0pYXGpBcyFxSiVepNJislqxqIfpPHiEXbsGUd/yKT6+PItck4YEYkR8Y7ibX+FXv+shYe5q1m5cwqIM3RsJdkISKv8IY0Md/K4lnzs+sYJ5BWaMMlhKkvQBEvMOM9Jex6NPnSbz+ltZMTePYotm6tnLwcPIqV3s3fo8v3t2N8fqRKxvHMens5CWm0Neesq7q1MMWvSJEAxGRJ3ix+9xMjbaT1/7aWr27+fYqVpO1TbTMRYjkFnNhgWZ6NQqeatbep3s+ZWmxIhF/Yyd+CM//+ED/H7rUU67NKiNi7nr23/Dlz68gtUlFgxKT+rFCk4yMeFk1O4i6HPhcIwwNNBJe10N9SfqaRwZZ3RoGFfmrdz4yc/y7a9dyzzjxfS6xgj17ODpXzzNLx53svRn/8FXl1nJM4noGAviHTlN8x/+H5+/38yKz97H5+5bx7JUFUG1Bo1IcBNUPuy1L/Pqc0/y5Y47+MPfbWR1lorIpIsxr0ieEyzkZiWj16pl8JQk6ap15Te5CDHZtotX/vggP/7p0xwYcRPVLGDdJz/Ppz53B/esyMOc+C7qlIifoEh4+4ddBLwTTDgdjAz30tveSP0+UXcN2cVzo4yrS7Gt/RN+/uN7WWLVYXo37yV9IMmeX2mKCpUqEUNmNpboKGMikJzqGiIc6KWhT0N6URHFxZnYdOoLG5JwNrUWnTEFqy0DW3Y+haWVzFu0inXXXMcNN65nZYVolbtHGG4XyXFiGpkL57MwQ3sR7xMRAXY/u/e1sXe4kLu/fD1VqUnolUQ96mS8+wRb//NX7LJez4abVrGxVIfaaWcwwUyyCIaJKicD9Yc5tbeG8ZUf5Z65ItEdPsWJ/S/zxPajbKvXUVZhw2LWItJpSZKkq9KV7/lVo01NI0UPOlc7OxqGiYSG6B6N4FOnU7G4nFxjIhd9ky0hUVQryVjS0kjPyiWvqIyKeUtZvnYzN99xI6sWFJKh9jPZZ2egN4x11Roq0/WkaGX3hXSGTH6ls4gIlKAjNSMFQ8yHr7OV+hEvuPqxR1LQWnOYV5aOYaaGA4gAlmiwkl4wlyWrF1Ge5sYZDFAXyOaGpdkkKR9n6qVvz03fsVfYXT9Ca9oqvvSJRWRpVOL3Y0RdPfQe281Pf1KD5cP3ceuGUgpcHbQePEJH9nxyDWp0oT7a9tVyeJuD7DtuoLh7F82TOtzqJLI9zfz3S2pWXTuHXJsR/Qz96ZIkSVfalU9+BZUGg8lEWkYyCS2n6Bjz43aO4vaFcSTms2pxLnp1Au/mpuKbqBJQJRpJySylcvECFpSnYA5388xIHiurbGSn6mQHhhQnk9+wB5/XzbArAYNyW3tGx3kGcdld+AMREvQa0Qa+GiSQKFrUyfoEdOFxmo824gr6cY2FiCaaMReVMDfzzIS0Sz5TKpWIVSIB1ugxmK3Ysq2YIuJcDThInDcPm0hglWEJ7yjYTc0zT3OgxUtwyY18YWMhyeLXlMQ5NNZBR80J/menlQ99+XbWlwSZHBjh1EAKi1aWYdOKF47UcWR/Dc8fdpNbbiIpJYfc4jlU5tvIsqSSXDifDYuyyTAmvb8DZ3gSt8eL3aMmWemhv+Qv6DVn1lN2Dk8QjIrzqk26Ssqy9MGgrNYSwOlwx2NpooilM9l/FwsFRJwYZkylE/E/4QM9Meo9SX5FTZGQpEdvTCbb5KGjvgO7YxynK4DHGUNbNo/idD1mUf9e+pkXRxAJsDpJiz45FYstjcz0ZCKnWkkpLyHVIuowpVfl/Sg4zrg7hCd4JheZOVGiYT+OASfRpCTxXcjhe4oPUPKrfMEextuOcEgU9IaGdoZH7Di0aVj1IsGaJqDF/CP0tXXT2j5OLD1DvE6NekYDXwS/vYuevlG6HSqstmSUIUfv00vvDQkadEYDyUYRjIbbaBx04RwbZ9IbI5CQRtn8EiyaGa4kxHvqU6ykJoXQekc44sulMkNH8oUERG8bx585QZcrjaKbb+SW8tR4kqr8XjQ4IRogo9TWeZmz1EJs0onLp8NQWMnSkhS0CRHcHYfYf7KBbT1hig2jDKtyyMrLpzQvk/T0LIpKc8m3aMVr3+6TRESBctFz4ij1Ncc5fHA/e17ayc799bT19dDZKZLwtlZaW6ceTac4uns3e7bv52BdDw5TLjaRXOuT3l1YivkG6WjsonvAS0KaUuZnMvlVRPCNtNHR62R4MoFUq/HqKMsXJErY78YlKuSJST9hvUE0uj4of9tM8OMd66Pt4AFO1p+I76S4e8chEWeb6Rrso7O9lbbXynVLE80Nx9j/0nZ27TlB42iAoMlGnjlp6lgXK0I05GLw1HHaJmKEdSbSTNqZrbyjASKeARpq+vCgIdEgErUPzNhQZTdLH8MNIi7V13Hi+FGOHT7ItqPtBK02dJEJ3PZxBh1RzDmWi7jbdpFUahI1BqxZqURHuukbttM7PIbHNcmwx0JRWR42ixHjTCamqkQ0BjMpqWYyJptpJQejyUx2imaGru2YKDfDDLSe5vDek/QodeVQlEiijhTTRXR2RUNEvf3Un+hkxKNCk5pCqm4mk19lHXs/vqEmGnt8eKNJmMy6ePyezT4wyW9MJL4Bewt7H3mWnc0DtDbW093eSY22gsX5JgwiqXjju46JmOpmtKWW2toBet0GyhYWkCJeM7ONftHiDQ7S09nD6bZxQiLYpBmSSFKr3ucVq/jcGiMGcyqFtqCozDoZcjoYHXUy7giTUDqP8uxkTLrEmblV9RqVknQnoDGraR7QM6/QFF8p4h3fIiSCd1cCxuy5LL92IXNSkqZ+R4VatHQ1Oi2pgXFUZhWTvmRSM4tYvqIIi7j6VaK6Gzy5g2Nt/bSnlHBNLowNjzCGaKhYMynKMKMJhdBoRYtZFI63/izK0nAuug4fpbHhCHtfep5H/vdRHjnuIsEE4cAkEyIJHx2degx20nJSNNR27Gbv4VYcldewLD9ZlI+L7FuOicotOslQwwlO1onGXtREydwcUuJ/20xRjiSujYBIgFpEY7HfQyw1DVtykvj+3+9l+W3EwkSCLhyDXbSeOMzRI22022Mkl+RiFedvZmPBVSzmx+Poo/3gQRpaj/DCA7/jsWcOc1BUpIY0NT7HKI7XyvXIICN9bdTu382ul/fSMCGSybJFrC00votyIhIJv2is9tRyaH8LTnMO1pwssgwz0UP4BpUqKh5eOg+KBMYZI6RNxmrRi0Twg1AAlOTXz1DtEU63dtLcPcTwuAd1mhWT0YBeJMY+b5RA1EyhiBsGkflenrs6Ik6ok0g0Z4nkMyBi7BDdPX2MuETsbrGjLigjN9dGtkU3s+ddJMBqjYb0rCidTjNWUzJZVt3MJPhRD/bGQxzZtYeth9rxO06w/XgElSmNwiLrha0WFAsR8jkYOHWIw/VeVKk28ovSSZ7RilWcBlFPqAOdnDjZy6AHElNSSDOKOm3q+dnoA5L8ihaYq5fhY0/x/77vovLmFczNnqS/sYmnxkq4daWozPRn3aoVlV54ooGdj59gNDGDypvWUW1+F4Pu35FIvoxppDBCcLSOh09q42NmzToRBN73cTWBRJ2BlIJCrGMtdPQO02Mfxu11UN9vZt7iUrKtRpLPaVRcOpUmBXN6IasqRXC+kMRXocmkZNUSlqyrokQkvudQ69Fb85m7aT1LFy1h+cIKykVweX3Wb3SIple20TSsJ+O2L/PtL9xAdvfjPHPaS1vEworiRNqODWNQkuC3vd0vnlGZyKpaxIK5aejHB6g90E/v2q/yj9/+NPd9+AauXbeWtWunHhuuY8sta1lSqEMz3MN49U2sLxbBWX9xVU8s6iXirGfrQyfwZc+hatNSKkwXeN4uiorEZBvWSLdIFlt5vlnHwsoM9IkzNFbvvRB2iwqriYNbH+XhH/yQ+59oozFcyLpb5pOjEdfu1fp3zTSlUZqSTdHSFSyfk8jAvpO0BYvIuuPLfP8fPsVN69ey4bVyvW4D6zZez213rCDHOwAGC/rK5awtME4d7CJEfUz2N1P73BPsz7mJlQvmsDBTP/MVtkiQEpJSyM92UbOvne7BAGalJ/LdTO5931HOlg5ryQKql61hwzU3sOX2j3Lfffdx3123cMt1m1i/ehFLqnMwX7bE9zXK2UzCmJ2FJTyOd6ibk53KBLhu2h0mLLn5lJZmkTYjwx/eoErQkGAuZl5JBjkzlfgKMW8LBx95mn214yTecie3aBvY8bSdqDWH4sXi+lCG1L2tmMh9x3D21vLk/57AfM1GUa+WUaC0QGaUaHgowwtTcsnwHed01yi1dmM8F1HuZs7WMDfjceS94WfS3k39/r3UlhVhME0y0TFCZ4eeTYvySNOIL37qlUoPXSQ4Rte2JzllzEBVWU215XKO5EzEmFdBUVExC489wv7mYfrdV3qx73dJpSVBW8jKez/JHevms9SaRMQ9zOSh+/nlH/ZxuNWB92pfKM8zwmB7ItGJLOYXWEUDKAmtXoPJmoxZ4yM03Mi29hAef+wCL5YYwTFxTIedDoOZshVzyDPpMEwTYVRJaaTnlbFoeTGFGcloL3rIQ5jg5BBtL/yek5lzSJ4zh8rUy1mWkzCVzKfUZqX0+NO83OLE7r3Sm6HMoMRk9LZqNn7kbm5aV0lZhmHqCWk6sWgY32Anrd4gvpwMSqpyyRRF9k0lTpl0pBeJzLwy5lblkKFM9b9oMcLjHXQ01nF/zVzWVudTnHEZEt/XqNSo0xexZl4QdaSRR/b1EhA/VvpNpRmWmEn5hhu55fbruTlXF29k+k8/y9ZntvLknk5cyhDvqZe+f0Xxd57kcJ2X0+MZzLF5aNpxFHeRDWt5Bhm6C0kpg6Jx10Lz3lc4vmAzhUXZ5F/UEp8XQ/k8GizVq6lQe0k+9SrburwEw1d7Bf7ufTCS37AL12A7tXua0BZkklG0lGs/9nn+z3f+nD/fIJJf/Vk9YcEJ/AM1vPB0BLM5j4pS27tbu/aCKUMIrKTlFLBiuYptL9dQ2+lg8qooc+K8JGhF8r6Sa++8lZs2LiQvGiTi6aXuqcd4cftJTvS6r+oKIjo+RG/YjNtSyJxsJfnRYitbSGF0AteR4+w4OEbF8nySky90kk0Il32AodEJPNpsFpVnYxXJ9LQpqVLhJuoxiCTZakqadlz62wo4mOg6zROPqMkV5aukwILust6rF2VZm0FWYS6VlSGeePxYfDy452qNn/GZ4WdmohvFd6R5/9+OeQ9FRfLrY7SnnSFPInprDhX5VnTimWnPmmhEJmmS47fWTRd5NyMu5mKouYHWU22Y1q0UDS7RGE28nNWV+CvUJrKrKsiNhAg9v5NDIyF8szg5uHwS0aZXskAkwB/7+HXMUavRBkbp2v8qO57ZzkunxwnHYu/jBFip8bwMtjTQNSEagtYS5lQsYvVX/41v/9VHuWt1IWkXEEtinn56GjrZsU3PsuVl5KQZ43MNLh8VakMepeU2zCYXTzx5kl5fKN7Im40+GMmvXyQqQ6M0tyWJBCCLzIwCSqsXsWrTUhbmGOM7u5wpihECk8MM1O9jV6yAtOwsiq0Xs57su6TSoLfYKF4+B8PJY/S29DPgvlp6zMS509ooXLaZzTdfx5Zl+WhiASa7DrDzpW28tLeRrsmrpCd7GipjEQu3bOLaW5ZSER8rnIi5bCPXrl3DlpXzKCquYGV56oUPwcCNo7+foV6v+NoLqC6yYNRO/W4sSCgwwcCIaHGHXmsyJKFOMJGaLJLfi2qERfCN9dLTfIJXEyspyLORN2MTOd6GSktyRhaF8/LQ7d1LR8cIo1dz7690gcJEwk76mjpwThiwpGRTnGV6owKJeHG7J7E7/VM/UOjRakTye7Hj2IXoRDetDYPUtupZvKYIq0ETn4x1eanRZZSSn6ElI1jLCzWjOP3hq6AX8uqjSjKTVrqYVbd9mHuvqyBbxF7/yGlq92/j8acP0uAQSeX7tuEhYndsgsH2bpzqRHQleRSkZVG06jrWLa+gXFwX7zjiQVxP7oFmWrp7OGaoZFlRKhZxnVz++C2u3eJ8cmx6Yrv30TDgYTIwO+9vfACSX2XGpRPnqIs2Tz7lubZ4oJz+DwvgHRuk/Wgt7oVzSM+1YL0iy56oSNCZSS6ax+qENsYG+ukcPbuSeL9ToUkvp3r9ddz5kY0syNKjSxyh7dh2Xtm6g5drh3CL6+dqvIRUafPZdOsGbruhkuz4nsoJaGzVrNp8A3d/5EY2b1hIUbKaC14bPTTGcFcfw/0htKlFojwa0U8152Mi8XXbOzjY5sQlKlXlvZJM6VgLKilJ1YhzejFl0c/kcB+dp1vwrlkgGnym+CS3y0+FOtmKJa+U9dFa+nqH6BsPTj13BSiT1HwjdNcc5MCru9lzoIHWIReeUJigb4Kx3iYajuzlle37ONTUR/+YB38oQiwWIRpwMd5dy5GaFk73juOernKdep1ynHpxnD37xPvUddLnDBCOnvv6mDJ3wONguK+brrYWWpsaqW9ooa1vDKcvRPzwsaj4yH68E3aGezvoGRhiwDHO5FA7p48fp0Y0hPtFwhhvPoj3Pud4zeJ49c209TreON57JRok6h+ms7aXSZ8Za3oWeenaqSfFR/cO0DfQR0OfZypZVGPIKCYrO49scf1cnAj+IfH3D3ppUVWwpDD5Xa+CctE0aWQVW8gp87HrQAejHtFgnXpKmklKHMkkc95GPv6F21kzLwebcZLR7mPsfuYpHtnfSc+En9B7WebfirhOCTkY6rAT0upIK84gTRTxi4u+HuwdbfTZ7YRXzaPQlIj+ihRxFUmWHHJtFpb4azjRPs6Y5+rtvLoUV/GEN2WwuB+fz4uzp5aTR07y6DE1q+++hqpMfXznLhLOW7os6sTe1sChPxwh+bq7WVuRRbbx/F6JGNFwiFBAVFiBoKhQ/aKNJo6hrB34+rGihAM+AqLCDYkgr6zeoATskNeDxysSbFGWNJrEcxNw5XfVCYT6t3IolkdiWgFL85OvotaHaOGmitZpugnrcA0neyaYGO9jRFTcQ6FMlq2pwCqSvAtal/eDbLKRw0++xK46L76FW/jcvQvJEpmzRhUlMtbJQMsRHurPY2mhOI8GrUiQ00kvqyDfoCTYF3HuIg76a2s49UorqTd9lGtKUkl70/I4oiyHRBkOBvC9XpYTRFFWJqmdVZb9XpEgiv9WkvH4IaIEPZMiyQkRiIpgef66kOL3Y7EgkaHn2aWeS3pmLnOzDFegLItrPuLDP3KYp7/3j/zge3/kj4fsUFxGaY4J1UQnTTv+wIM//S+++aMXqVfnkG5NJTPViFEjrs+xDuqf+0++82QvvdEUysuyiedvMSftO7dxpDWMO7WQlcv0jO59kkd+/TN+9cRudrT4SBDXq/IeukRxLpRTFw3FVyIYbj/F4UMnOHXimHgcZc/+GpqGomjE+6ak6NEjzvvkMN31h9m3bRuHu8YZ9AVIanmF3/7PQ7zUk4TKmsmcXAPqgHK8Go4cPs6p48eoEcfbLY6nLJ+ksaSQYjZiSJzZCaYXLOQiMHyKZ3/yBMeTFlB57XpuXleINR77ogS6DnCkc4w6fyZrisV3IeJFcl4RWbk5ZLxzN9hZlGwnwPCpV9g9HKCnZBWfX5MT31zm3PKllIUgAX8Qv99PUJRzXziBRBFjVa+tQKI0ZEQd4faJZEX8LEE8J4I7kYCbCRG3lPKu/OyNa0GhJjEygWN0hIe3hth4bTnZFtHYv5g/QbpA4lxrjVhLizHZm+ntG6JrcBjPxAAnBtOYt7BIJGkmUZe/R2X+fErjNCRi6eQ4fnsD2367nY6kQvKXLmF9rp5QVJQlUZ4SLmS1isggza8eorMjRO61t7Ax34jmTXf+YkSCfgIidvtFOQ8GRANcWb1CxN8zVYW4BpTP5BMN/IjyAyVHET8VZTzoduHyR0RWIq6J88+fWi1i1xAexwFeiC5hWXEauSlX4A74+8xVmvwqia8Xd8Mz/P7Rp/jD48+z41A9LRMRtJEuWo6fomM8Siwtj6KzZ/97+ug5Us/TD7go+8RNzFduNZzfpRfzM952lOOvPsvvXz3Kgadf4HTQhNpiI9es3BYXwTk2Tsf2x9l+vIOaUBYLRcFXhQc49ccf85MHd/CHNj0rVhZhFBfBG+VZ/CMaJTR4hMdajISMWayclzntRKj3rQSRrOlNFIjKbaSmgUG7i7ExUSlOTDJinsei4lRSDbN5A4QYod6DvPjiXg44Tdi23MEX1+djEoUgIebF0dFE65FGwgs2sTjbgOkSBnjFJjtp3n2aHS+EqfzUTVRn6jCdfxcj5sPesJeDO7by6LbDHHjyOZqScjBaLGSKRp9K6W8UZbn5uQd5qcFBV8wmGo6ijEf6Ofyrf+H7fzjOtiEjSxblYRLR9o2ji6tAJBXh4aM82GglJTObRWVpV2D3O/EZRINWWXUiJTZAR4+P0YRCVt91E+tEQzLFnEa6WXy2CWVzlnFy7riPD60opswmEksR8FWiAatxtjCSu44lC6tZlWcU16dyC3Mq+W0cotOfhN6USuGi9ay7Zi2LLV5cNUf43XOD2FYuItdy5jyHJnoYqnuF/z1sZf7qVWy8Zh0rqvOpMHTy7A9+xf4xE4k2K3kpAUaaajj29K/42SM1NPQHMaSKRm9aOhbRUKr352HNLRDfn3if+pf4xWELVStXsmnzepZXF1CZ3MVz//5r9tuNJKRnU55nFg2pqdNxBUW9o7hb93D/7/cxWLGZ1detYUtVOlqVEg9ddO7dz6A7iZTKBcxLe7dr+iqU4zlofuV5mkYSMM5bz60VqfGY8safLeK/aHz4e/ey9YVdbN+6lcNHj/JEVyqVJRbMepEgiIQ85B7EfupxfvLYCBGzmQzRQFONt9Oz7af8n6+/QpvKQmpxNtmi4fkGca1GXLi7+jnx8yYKbllOfqaJlCtyh3AWUolzr04hK1tHYGiA4c5e+iZEbOnvZjKljNy8LIozzqwp/p4L9NFxbAeP/vdveXLrSzx9uB+7X9kgpZPTNTUcGksjP8uM5QLmh8TG6zn4RCtNPVbm3rWeudbz9yJQ4pKXgUPP8+rOnTz78l4Ob99Pa0oFOanKNs1KmVUa1j3U/fFnPN0rkm+9iFuWCH5HA7t++Nd88+kRBtSpVJTZSD7n2CqiHgdjg6f5nxobKxfmU5yZzBv3cWaHd1/7vqdEJajWoitay7W33cIta6uoyLWhWXgTH77vPj71mXu4beMiqtLP3bkm5nPjck/SnJBGskmL5k230kTyMlxP07CbDtMybr9xBfPNnRwRid6+llG88ZdERKxv5cTLBzi0r4UBd2gqKMcI+104mjppP9BGp0sE33PGAYhXqTSYLFY0o178g64LmvQW8/TSvf9h/vmeD3HPhz/Ehz50sY+7ueOur/DtJ2qoG/YpVcslEC1LQzqplTfwUdF4WFmRR4qoKCa6T7Lrv3/Nk4e66BybrcPnlTMbYryvgy7nGP3OvvjSe7/+yX/ykx//kH/7x2/y7e//kZ/sN1CUrUd3iXvMx7wTjHt8dCZasYoGXtKbagdRlgdOUDsaw25bzs2bq6nSn+bVIw0c6xzDp7wkEhB5XyMHn93HsWNdDHvj9zjO/K7PyWhtGx1Hu+h1i4bkOQVH6UXTYk5LI7F/Eu+oB/elFayLIJJYtZnc6iUsKDZi9XdxrHUUv3KtqTXos4spnlvC8nwXpxu6GfT4CcQDf5hwxEd/p5HK3CzKck3TbNIiEui0EhatXc3CijIqKhexdNlCFpQk4+9roKFH2SBFNBjC49g7mjn4VD3GghzSM9KwiOQqxSYSqfJyqtMc9Ow7ytEjvQzFUrEVljGnyCyO72ciwYg2bz5rV21gy+f/lr/74i3cuchAoKuZ/U/Woc/LxpYhEmPleBnK8SrE8cbo268cr4Me73sxuChCUJS3IfE3twSDjIlydeKlh/nf//wRP/rhD/iX//uX/O2vGtgvKuCc82LuuxJ1Md4rYtWEOr5SxLmJrxD1E/F2sGeHC0N2NWs2VFOUNMnBB3ZS1+fGGb9X7sfr6KB269O82NhP12RQaeqJoh0lGpzE0XOS0x099NjfPPwsQatDn6wnV9WLc8KHV9nW8G2J94uN0vT8//Djv/jkNDH4wh53funb/PVvjjMsDjd7bkSLb1aVhCFfNPhuuZGbNy0gMxYSX18XJx97jOe3HuXI60Np3mNJ6WRVrOKGj93FHWvyMSVXULb4Ru687zN86uN3c++6ongP6tlNqbcSc40x6I8xkpSCxaQss3puLIqFfAS79nJg3Iq2bCXXLs/BFqrl96+epsXuOTNJLejC21/Ltod3Uds8jNMfL+Hx4VghkdwOHGqko2GIoTctyaQmSavHbEohQeQ1ky7/mfg5y1ylya+QkEhSagGl5UUUpOpEtaXGOn8py5YsYemS+VQWZZFxToteqdBF8uv1MJRiw2ROOm92t1JAxHMNvUw4xbFKS6jIiOHqHmTM4SYgMtn4q5VbCkMt1LZMMjKpwyZaYfE+sYRU8ubOo6KgkNxwgvh44qfnlGflfyditKST4hYB1zGJK/DOl7RKLQJxag6lC5ewaPESloi/7+Iei1i8eC5zMk2X1NP4ugQN6uR8qq+7nZuvW86SUrM4r8MMN7zIE4/tYF/DACOz8UpSWuqi/Ax3dmIf05KaOY91qxaKpKeIosJ88iwaEjVJODS5FKUmxidhvi4qgr1PtOB3PMUff/prHnp4B0cGA5w3xPQsoix7XDgDAezmNKxmcS2cc8tM+Sxu+k514wnoySgpoCQ1jKOll3Gnl2DkTFlWxqIGB05zotkvyqJOHEcX7x1VynLRggVUZOWQFUlA9abEWkl+leXgbKRMuImMuZkMXsnqSY0+p5zKqizyDU5qDzbTJxJ35XJKSNSg0SWhZ5KBg6do73UypjwR9oi6YoCTYzlkpKaSPd2OYwl69CYb+cW5ZJiNGA0pWNPTsaXrSQyO0DsqEv1gmKh3mKH2Jnbs6WZ8qJmm2mMcPnCAg0dPcqJlSDQERJLb00ZPxwADHnHNp1hJsxpFY1uHTumdzsmnUJzb/KrlrJhfTJnJy1h7Mzt2ieMNtrxxvCPieM3KeHo/zt52cbxeesbfixGoIQIe0aBr7sAZLBCNgoUsWziX0sJCCkXyn20I4k6yEjOkTTO+N4R/opeOvU/y6P/+O//99B52tY+/dXKntLL8omE3qhJJajI2sxJfzxYj4pvA3XGCNpWNVHFt5Yh2hd8+GO8x9CtjrJWXRUSDbLSH+r3NhJKNGIzi3CtD1wwWrFUrWDnHSJZZGcJyftkWNHq04n1zzKOMT7rxTiUVb005hgajrYDCqgXTxOALeyyeN4eKXLM40pkjziYqnY2ipRvZfNN13Lgki4QEUeY7D7Drpe28sOs03Z7Ime/1vaQ2kJyeS6n4nvJNUdTmPNE4Fd/3CvHdLahmQWEq5rMnRotcIeoWsWJUxGpP6KzPHyMgkl+HeKU7RSS/4po5txgqG1846djfTsSYSW5RJhlJPkbb+nF6Q0RExaDcgY6K68DV08Le00rcM2AyKhOeNSTpM6lYvZzyZFHWY8rQnvNLkyq+257ZbCFl2I5vQhmq+Z6f3Svu6t/kItBL0/bdHDtuJ3zN3Xx8cSYW7XR7V8fwjzRzuqGVp1syufsTiyi0iMTy9XIhvvzYJP0NDtSmNApKRNBt3c79P6slVr2B1WvmsyhLF69Ex04+xx92BYgVLuL6WxZTogR8lRazMcDYuAZfxMb62yqwih+fXRXEd1mZEJXcKyOERKGuXldJ/jvdK04UlXBGMfPXb2L9pk1suujHBjZuWMHCAgupZy/59q6JI6jUaFLSsWr94iIepU1UyuN+B0NDYC4qpbA0j8J3vZ3phXM6nbzyyis0NzdfsYfP5yMrK2vqE5xF6a2IDnHy0d+xs0aLacFtfOEvP8lty6uZW1UhGh9qYhojDn0Zd6zPj+/g83oZFS31WHCYxp3P8sRDhzjWqyZz3TKq0zRTY7vOF8U3UMvx+j72juTEy3J2fFzx1NPx4QyT9JwaQy+SrLzsMJ66Xfzv/9Rj2ngrG5ZXUJmuEcHTgf34M/zqVTVpi1dyzTVVFChj4FU6UpJ9DA5pSUzOYdX1pVjEh33jo4h/RQOoXc28+KwDfUERc5cVk3MlB0YmiRRhsoe+zl5216mYf+1i8s0adO5eelvrOHSyh/4WD7bFCygUlUeWqGqc3bXscJaxsDqXgjTd1LUpGgqvDXtoEzEiewFbbl9A7tQmF1FnF+0NNbx8sB/jmhvZVCmSYX8bdYeO8fsdbgpWFJCiDsXH3Xk8Xry+IIGwGl16EeWL51NVVUiePsBE806e3TdOQt4C1mxaweLM124yRgmONFJ/6CgPb586XpKo4KaOp8whCE4db85C5XhF4tq6+NUTLkl0kvGuGg4/8iSv9i3ixs9/mns/ej3XiEZ1ZXkpZaluemMF5JWUsjo+FGLq9+JEo0Q0OvpPvsKTv3yYvb48zIXlLC80nxMb3yC+j8lWDj7ZiCMxm6LNK8R1cHYsmUp+u1sYLlhKmU00NE7sZvur9bSW3MB9N1eIxqWOJJ94z7pjPPnAKYy3fZzrlhQzR8R7dVIiWksygdp2kqsXU1BZSv75CbvKj0ckzm3Pb2dk2RbmlohG1tnD56ajXDN55VQtWztNDL6wx0bRWF5Semb40IV2U+zevZva2tppY9XleAyJIF8oGj3KuOqZlSCqOjOmZA1mxqhv6GAy4GTEIa4tTRp5C+dRFY+HVzDGTEtZW32A1m1/5JmhMuasXsqm5XlY3lSYo0TE5x+v38HOngQiWiOZSvmbes7TuY9tJ72M6Iu5+dYKbEqfQ/w5hZL8uug4Nk7mokqyNAOi8biPx54bJvf2j7BlfhY5oswGHJ10Ht7Bz3aaWH33taxekEeGMuRHnYTV4qe5JZm8qmIWLM3GfM5pU6EKOvENtfH04w7KlV1RS22kvRfjqd5DF3qNvW/FXHZGR6KM+bOZV2jDmKh+i6CqjBOLEIpEUG50vblvUvni9WSuEC2mhWVk+ofpOnqYw95y8ovLqMpNEceNEI1M0NPYxIjaiDYni+yzNsiIKUlhnhXL/FyyxId4yyoqKFqx4qG04K5aKjN5K65nyy1b+MjibDTiT0mcaKWrp4+uEa/SlLjsenp6+MpXvnJFHw8//PDUu59H6b319NFe18dYwIg5J5c8UWmfidWJ6IypZBTkUDQnm1SR0Z5TRhO0qJIrWbV+PsXzFpCWqawSoX+LxPeM18ry9INMlMvaQPb61VRUicDs6qfjVB0HwouprCihJFO5TxIi6B+jq64BuyGN5BwbGSlnldgENdqSDCwVmfHA/JYfRZTjaEiU5XPHRVx+KhOZxaJRUZJOcu9RDtaN4Jj04BroZ9wbQbt5CxvNHXS3ttPW58A17mS0tR/bkkJSLcZ479q7IxoeEw5GxtwMm6tZseU2bvnwHdxxh/L4CHd/7E/4P//5IL959Mf801dvZF2ushLu24nidynHm2RIHG+ZON6t5x3v6z/+LQ888mP++Ws3sT7vnY53GYRcuEd6OX1ymJClgMwsC2nJSlkRlWiChmSrjbw5eeRkp5L8poKixZiWz7zrr6PaXMbqwnzKs1Pe/vxHw4SDMcLTdg+LRqPOSnLpdVw3L4PscC9tDT2c7jew+oaFlKToMSTECE2IeqFngGOuYkpF2U5PPntCj0gQkkvItqVhM09fWyj1hdKIDIVFEnOly/YFionP9a//+q/TxqnL9fjud79LNPrm2nNmGLAULWLVHR/jKxtLRGNWNGK8g4wra/h3T4j6cupl7yVluUoRN/uahghnWUkVZXn6PbLC4nV2ug8/x7ZTrTSNes8Mu4mXK2VSfYSgMgco/rPzJYqGQBqFt1xHVb6ZpIFW0fAY5HTiUjYuyifTrBURPoDHKXKU1g7GM0uwiWsv9bUNMpRKJyERvWjkpxVY4h0X01I+SiBMJCQ+z/uziF9WV3nyq+ymNUz/ZAy7Po95BSKovuuR8eL3lDXwbCmkpEREy7+TU7vqCCxfTuWCfIqsooQrM9yDw3Qeb4sX/KzK7Pg2qGeIgOscF9dvAoayLEziJ1d9y+IdqLS5zFm+kdvuvpH5CckUb7qXD9+4hmsqLW+dLM2gqqoqjh8/fkUff//3fz/17ueKhZXVDzpoHArgzM7CWplDrjY+IEZ5lqg6lZzMPNZV20gUwenc86NEnhCOng66RIveLRLkbKMya/3MsxdPyVaNpGekYjaJz9PXRv3xdiLXbGL+3ExylSQ35ifoGaT1UCuqUvF+JTYyX5/UI64rhwNVqkhcijNIFj+5Et/nxVGjyy6hfG4hmzI6OXaijT5HNy3NHsZGC7juri3ccEMyjoEemjpaaR92cbo+nUUFJiyXtItSjEg4QMDtxGMfYNAh/n1JQz7E8UJB/JPK8foZtvvxX9EhJO8spiwlOTjIqTE1qsVlZOeYz6rwVaJsZ7NwTi6VeebpY17QR2hAxNO2LLTadDJEuXr3RMNRIxLcrBysosXt7jpNXZ+fXvMybl2dT6o+SXyGEJ7xIfoGRuhKmU9JoTU+rjJONFJjgVH6NUUYky2k667eKK30vv7+97+fNk5drofyfsrk0csmSSRxecu46ysfY2V2HmXVm1h78+18cnXWeUO73iMRP6FJkXTWO7FlplOQkYJp2o+VhNZcyoLP/ZR//9Pb+MjizIuYUCaS38RksvIs6LUTDDQ10j7gQnftelaWmUQZF+c/5sZt76OroR/D6grys0Rcm7o5EYuECQwPk5RvITkrlXezv+JscBXnZ0oFEcZlH2TYE8FryiI3Xfs2GwUkkKRVdhsykBb2n2ntvKklqSJBWUYkMM5ETx+nTkBJdTklmSmYlW64kI/waDf1dSqsqVmU5linejqUz+JjpHNC/C4UF6VOe2Jj4j+hoJ+wUU9iqgHjBQTemKeHrr2/5R/vvJk7b7mZm2++2MeHuPX2L/HNx05ROzTDPbIxHxMjfXT2TzC86BN88nO3cf2SQjKvUIWSlJREZmbmFX2YzcrEpfOJshT0MtEtkiyfRiRlORTnp511q0mUK3MuWbnFLJq2R1cZpjBGb2MnGoOG4jmZWIJjjNc+zzOPP8D//n47Ww/3n7WVdAIanRGzTodFJGLBaVvuyrhckUB7HYx2DtLclEj1yrkUW5NJVr6egBv/cD+nTiWRk51LkQjiZ1YeUS4KL4OtTvQqNflvldDEovHlAEMmIxqlt+3ybk00rQS9DVtRKQurU+g7fIL6wwc5EVRjL1zE0rxilm5Ygml0kL4DBznWNUBTXhWFpiSMl1SJinOvNSIuYRKDrdR12HF6plnnOObF6RhjaHAi3uPz1tediiSdiAXi5Ccpx+scZdw93brJPpxj4wwOnDnelRPFr/R0D4/QgZWiinyyUwxvDBdTll6yVVJRkEmhdbr+3Gh8stxIZyP1ovwbC9KxacYYbtnHE7/+Dff/z5Psqetl5PX5D+LAOhNmSwI6XVg0BKb5a5XGo7KkFC6G2toZ8kbRzKtiQY4WbbwBJ8798AB9gw4iS+aSn/HGyipRvwd/dyPDhdkkWszTJy5R5W5GFF8khdRkPboz6/+9DfHZYyM0PvsT/uPP7pkmBl/Y47bPfpOv/+oYQ+JwFzqy22q1ThunLtcjLS1t6p0vk1hQlJdROk810Je9jlV33cm9Ny+i2Hje0qHvlYCXwKj4rgdSsKWkkWE6MyHzDWG8A7XU7PwjP7//1/zHj3ZQ1+PCH3ttuTHl/yegTRZJbFIiRmUpShG/33RzQZRxZdk0XAP0NIkY7jAxf7lIcnWJZ4YVeccZ77XT0mqguqqQHLN+KrlW7pq46aodJ8NiICN9+iUoo8qdw3CQkDUFvUnUWbNwNZP3RXl693yMDw8yHo6hys4SQVlZA++tvkQViaKSMYkE2OacwOdVZn9PXyXFfJO4HE6aJtKZU5JDpihYSqMqFgoQdgzRPG4mVVn6zGo8M7RBJAL4B+iwa0XRS6M8Y7o2nhIgI/g94thGLVGLSEIuYIyNKtGAIb2QqlVrWLVmDWsu+rFaPJYwLzeFlDetA3spgrg6j3PoQAM7ezK54Quf4Ja1FZSJi22WDR0SwvGd2/qbmxjxm8nIzKE0yxwvM69RJRnQGZLPtNrPF5/wJspPnQ+RVpCnc+OoP8FhexLhsVZqjtWw60Q/40oRiv+CKMt6kfwmarBOuJj0Rd9yCI3Sazfm8NDltbGgMger8cxWy7GgD5/DQcO4lewMG1mpokEW/4Ww+Cy9tIwmo0myUpw2XUKjNBxFWZ6cYEIkQirxMLwXwVMtzmdWEdVLyjB37GHvs3V0uzRkVhVjM1rIW7KKhcmDDB0+zt6jDszziknVJF7iskmi4kpJF4lAKkWxDk7uO8bpXtFQOWdFgIj4OltF8jVAr0+ZnKJ8Y29FjdZ85njF4nin9h8Xx3Mwds6uS+J4/a30D/S/frwrJ8ikiHkDvSM41QXx8a8Z5tfGLgpKIqpLwWzQYZy2AaRMlnPQ19KCtzwXk36csc5OTjU70Gns1Dy/l5N1/Qx5ppJc5Y/TmkhJi6LWenF532oDFeVicOHoGUDZL8ZWWUi+PuHMLnBRD277iKgbXKTNLyIrWTO1gYD4LJOT9NaMk1liw5JmOFPmz6ckBaJB43BaSTHo0WveKW7GPzSm7FLmLFpxXvy9iMeyeSwoSo1vGX2VV8zvkmjs2DtpPbKbX+6Agg03cvOWlSwrslyhDSDeibKik6i/h4dp9GWLhoAFq0gcz7kelTygy07nYFDEei+9L+2isX2UURGj3yAa0EbRsBI/MrpEWfUr3WLTi03aGXBEmCCD+XMy42seK6VRWe3HOeam02OjsjCDVIPmzDUZEcm5e5jaoQwyU1LiO+e92Zm7TV6fB6ctVVxuotF43uT/sGeCyUk3rg/w7m9X8TUmvpTYJI7eQTzin6biLLL0Z6+r+2YJOiOmZAMFgSGcEwH88SVxphdLSESVnByfAa+Jt/wj8YI/1ttDd0gUKGU5HFGRKp8jFvUz0dmGPSkNrS2PPMN0p1UZcywSRvsw3hQtSTYTpneKqQrlNmHVJu7562/x13/3Lb71rYt9/A3f/Js/5RNriilMnYmFrJVKJyxav80cf3Ufe2vcBIo28KWPrKAqw4jh/XBr6kqLBQl47SJ5bcMdTBfJZBYFNpEUTj39TmKRAJHRNk53qAlMBEkUwatj0IU3rZq5cwopzhYVomjxn01tEA0wvYYs/wijziDBt5itG/+pOknkicmkKWVZ6U0QZTnoduIQyVRXTCS9OiX4KYVRJNFBERDbmrEb8khOzyJn2rIcJRr24xwZwp9mRCsagfEhoFecBoM1l5JFC1mWpIw3D4nGYipVhakkJorrs2ARK+aI61Mk+j0OPQsrrPEgf2klVIUmNYf8sjmsK4nSv/85torrYM+pVjp7++nv66a76RS1yjJrkyESjPp3CLLK8bLF8cpZXxZj4MDzvKQcr6Zl6ngi3sSP18WAK0hC8jsdb4bFRHkYGWCga5yotoSqgjRS47PKL1DUh3d8mPaTfZgtYYITIwwO+vDrcliyqor89FRMScq6vK8RR040kZqZQFTnp3/cq0T66cVEQ0AUcI1eJJ5Kb7T4UYJSsXsdOIaGGbL7MKUq29srCYNosPmdTIyNUufIYl5+Kpnm6UcexwJefC6RWIQySTGL417IsoSqFPKW38ztX/w/08TgC3v87Z99gs9cW0aqOAUXUjV8sEQJufroOHGYV16sozF5HVu2rGbd3Gws75teSfEZvaLBNTxEtyafNJE4phrPTS6j7lHsfi2R5AIWFehxj0/iEa2z8Hldu2qzBVtSlBTlzpwrFB+/PS3lx0laNCYjFmXcenwsXBjfuJ2RUQf9CQZMytbfr8X1yXHGe3oYtc0jw5KKddoNZiKEAm6c4hiB/HSMIvl9vXGhLOXqG6Tl8Ha2bdvH7tp+JiLi2pl6+oPkisbRGaUUlpCD4XY7oWgS2coWg+KvebugoTKkYM4wUZXZy4B9Erdv+gV3VAYr6fn5rK9Q4XOM4LA7GFcK2/AArR0jJIomm8czjn1klHHnOGOjA7ScGCJZFKTcikyM016rEaJRNyPd/eiserJyLKRejWdfJL4RZSD/vmd5eusgDv18Pv75G1iRnoTh0rrUrj4iafVOOhkb6Wewp43aUwN4I2e2M04IiiA5NiGSYWXCzNTrp6UMmfDh6WjgVCiBvoFehgcd9KYv4/q5WZQt3MJH772ZT986lxxRXl47wyplGZtMPcXWfjoHPfjfooWeYLKRXZjN8uIozgHxfY2NibI8wtDgAF29DvTp4HKKZMFuF2XZgV38vOnoEGmVOeKaSnuL8WLKTkcuBjt6MWWnkJVpPm828ZWTYLSSUr6EG5dlUrJhMUWVBRQaxYlSGq/GAiqXVFF+zTKKllVTnaE5byF50SCNKNsii2THHyIUChIJ+fF6lBUWImfG9vr98d2VwkqDT3ldQLxOJxoF1au47Z41FHgOsO2x3/LLX/+eR599mW2vvsr2Z55ke7+eiCmPuekqgn4vbnHMSET8blA5vkj+guI6EjEsXjQMWWTPW8Xt966j0HeIV5Xj/eoP4ngvvX68V/u1hM354m+4lPGyF0ipAENekSgqSWQnXW1ddHROohKFxZwozoco806XOBcXsjxSaILJ4UEaDvvIcDXS3ubDp8lm2bpqMgqW8+FvfIZbN1cxR5lTESe+n4RUbEUWooYI7f1jbz1BOcFCdrW4LvKsxBxD9DqcogyPMdIl4vuYF3uCEY24NkfGxnGMi0d/F/0jw/SWr6U4TbntPF2hPdPrNe4a43ROKVbRoE++xDW5pXcWE/FkpF4kWy8d45WmHD79jXu4pjqbrPdHl+8UkXS6x0V8HsKbWUi6yCXM53w+EctDenJELpKfHWW07iR70uZhzhH5xHm9AwkpmeRkRUnTD9Mz7HvLyXwqSy5lpWmUpwew9w8yNi6uvXFR1vsGGRL1i9YSETF7mLF4XB9lqGdQNJadFG8sx2Z7q40rQvjdTuwDw2RWZsfX0n59s61okNj4Kbb/9j/59+//gp8/c4qegGhQXsClfrW5epc6i4nWkr2Glx88wVBSGdVbNrGxMPnts3lR8YV9o7gH93HAvJoFhTbyU6fpxUhU1vtMJjfbz6nnd9LU2UpdQwOt3S4m5t/JZzdpGWtqpammlpbOZk4eaMRZsYEFVUWUicR2+s7PAGF/H8ceegV33gqqFy9gYYbuze/9vqb0qgzjaniK73/rAP5Fm7nhkzdyY3nqNBsGfPDF7Md59Zmn+eNv/8AzL77Mowe7sLtF5e7to7OpnvqGHpwZVeSZRcPgTRuqvEa0wj0j9B18nEcHxDkMjxGbGKNrQkv2nFLSUy3iISpqw3kLoYtCFprsZXy0lt2GVawrTsGWrOxAeJ5EUXmnGEgzuzj66Iu0DHZTJ8pt26iIcwtv4ivXJ9F+uIG2pgaaWprEcz1MLrieFRU54nNrpy/LMZH4jHVy5LevEJx/A0sXlFOZ9h5tj6lSk5BwZjtzy7KNVM/Jj++TH+/fVWkxasIkZc2hoLiS5coWwlO/FqesczzRQ9OhV3j+ke0caRBlW63DqAyhsplIGGuj6fA2Xt6xm52tgzgTRLzIzSYnI43MzCyyKuazpMSGabyF2n3H2HG4l0lDJoUrb+aG9fNZWKAj5uyh9ehOtj72PPtOdzMYEvFGl0JuWjL6FFE5KavTKKvEJFvJrFhw5nhOEW/2H+PVQ8rxMihYcRPXr1/AIuX2r/hCLvd5jgXH8fXs5bc/+x3PPvUkT7x8gF2nh0QDy8nkRDsNR6d20EzPpyR1utuqb4i5eug6dZKHnh0hpSTKcL+D8YBW5PsFzMlKw5JmxmgQjZLXy7by34kkenqo6wzSOGTmms1l8bG553ZsKN+vDnO6BYN3iIkTr/JMTR+Dpw9ytNuAuWIpN20qImnPs5wcHKG55igNnW5G9fO4XRxPWRLqjfc8Wxh3Xx31zXU8Y1jN57bMoVhZOm26l0ozJIKv42Ue+dlODvVZWfGXn+PTy7JIVZYsfT+d99gk9tYTHNu+h305N/HxG+cyL0tcw69/RpVojFtITVER6T3Jka3baKi+i4+uL6Xi9aUVp4h45R+po987RoNpMdfPMaON996eS6VJwSIahgn+AY4+8woNojHXcPgYndEcMpas4p4lYQ4+c5D+wVbqT9WL+B4gJurlTeXpb33+YhPY2xqoefEUCevuYnNlBtmv1x0iCw86aD12jJ5wOimVy7lhZRGpIu580Kp4Vewt+9vf32IhD8HGh/nGN47imrOeu/7qo9xW9E7zGkP4Bupoeel+vnp4DV/+wjXcsiwXUVbPo7TglDVsR0QLa5KoTkuiUkmpE1Gb00hjjOERL/6oCq1eBP9wDI01UyQYImkWSc60ZSRgx9u7m+/+6R6sn/wIG29fyVLr21cc7zdRTx99Nbv43Q8f5nTOh7nhjmvZvKKIHOM0SddsEHQyOjImWuMufAFvfAOJSEyLPr5Bgg6dMsEyOw+bQSQ307eIBA+uvpPs/uev8xvb59m4rJTFtHH8yEmeSfkk3/34HPLFr6oiiaQXpJ/Vkg/h7jjAsVee4s8Pr+X7//ca1lekT7PUlNKz7MHrtIuy7EKVbEQZ4ZCYpCMx2Rwvy72DPiJq8Rk1ImmMJcTLcroyCSJx+oQ95utnuP5MWS7/9pfZfM085p69TNoVpfTehgiMjeDSiORQrz9nM5eobwxnIIGw2kiG6bzrLRaJD99wO0cZ7OrH7hIJndEaX5IuSySn2ohPnLdhhoZG6B8PkmDJozA3kyyLEYMysF3p+R+3Mzo8wvC4H5/47lPS00m3pZOWohfnT7xFyIdnYhx7v0jMJgKEtKlYbFnk21IwGkVSpVQq8Q8j3jsSPHO8kRFGxsR7n308EVt0munWL78MokEiPie9/WP4fB4mPUrPdYhogkg2rcnokjQYUqxY0kUsnG4M++sieFq3s+3FV/izpzP47j9soNBdy8Fdo7T4yvnyP3+I0vFuAikZ6MwpWF/PMGOER/bzwoMH2H4kket/9FU2ZybFGzjni4XF53PYsTtcTIrv2JQUFZ9TXH9G0fBR+3H0juAzmNAkxEgQDZtEYyqZ6cnxeQlvPpoQG6Njx9Ps3HqInUu+wXduLqAoVVlaSrocYmE3Eftx/vDvv+HgRAFZa2/kYx9eSlnKW61x/h4K9dL44pP88d8f58hdP+A7H6lmSa6ybOR53M2ceHYrD/7XYTR/9x3uLQiTKRp6iRYbGa9PiAkwduIpnt3RwAND6/j5P26mWCSgb17IUFwLvgkmHA6GR72oRYM5MRpBrTeh02swhp109ftE41nkKOKEJSSKeseSHo916mkbd+KIk42cfGkPD/ykjcX/+Q22VKSR+9qGYMr8pbCLwc4uRnwa1JZsSvMsKHPY329fx6W6qpLfiKuTltoOOuyQtqiK3Iaf8teParEtu4Y/+/w6KqYdn3g2kQh4Bhlr2cZ3vt9M+cdEq2fTwitQcYdF7ttKz+4H+e72Odx472Y2ry7EdjV1JwRG6Dm5i11bd/JMi4VrP/MJrhOJWun5LVrp4oRHGW3az28+8wMGv/BP3H7TAua59rLz6d/xfzx/wkP3mUlwaYmo0lizKv+s5DcWvx66T+7kOz9oZt3XP881K8T3cdkH34bx9p+kafez/Mu2cj73F9ezujoT62wb8iJdAC+Dhx/nxZd28z33x/jDN9ZQMbGNB39Vw3ONmfz1r+/AuOMEmvnVZJbmkX12PAwNiERjKzt3ttJy3Vf4q03Z5Ijk4HInoZGxE+x6fj/bjvhZ+Bdf4aYCPaly2MPlEZ5kcqCBQ4/9hicbjGStvoFrr1/NyqJzJwu/Z6JKB9ggLXv30WpaRGmmG9eh/Tz0k9MU/vCf+OSSjGk2c4oR7NnFS8/u47+eUfFnP72H1NO9mAvyyVxQRt7ZDbzRUxx89QAPPTfB9X//ZTYVpmC77CslBZlo2saenad4pHEef/mtLVSm61FGis02V9GfHMbde5x9W5/nD0/s5sTAMK0NdlLKi5mzoFC0XC7kTxEtI70Fc9EybqsK4B4YoK17PL4t6uWkjGdyDvdx9KiLrE0LKClNfx8N4r8AkUlGW49wcP8x9vUkUfbhj3Hd8hKKrDOf+MZCk7jHejne5sQdeB9saXm5BT34HWM0jBaTn2GLb6urVbbVzcwgOzTMYFM7g04vfo3uvJnpKtTJGdhK5nP3XGX8bR9d/ZNc7iVilVvio3391DX5Kb5tGcV5KaTIxFeaTsyDa8SJ1x5h4cICUbEnoUu2xsciZunHGWqpo8khkoVowrnbfSuS0smtLqFiiZnRgyfoHfPiudxbsMa82Fta6B8XIW/OKtYUGt5iBQvpkkV9TA610HBwB4/uGUWz6BrWrF/KkoLLkPhGQ0Td3TR12xlyBi68ThH19mTPCZ7/5W94fGctJ0+3M+KN4J6zhjVzLPGhaG8WIxoOExb1ezjDQGyolYHJKH405y2xqCLRUkhpWRHXFjg4cayHYaePt5mDPyNiSidW6xB9E0nMuXUJxSmvrYQy+1wlf7ZSIkK4Btpo6xmg2e4mNN5DQ1Myy6rK4zNClYX4L0iCjiRTMavuWkqi205XYxd93vBlTLLCBMd76W/v44h9GVuWllKWMbVE2vueOCuxIL7R0xzcto1dLX6Ci+/ki7fNoyhVfxmWNIuI77WT7to9PLhnALv77P3QP5hioQjhqAp39UqKCtNIS9ZiSCugqGIpq7ynaayZQC2S4YqKtDc3NJTbu7ZSNnxiOeGBHrra+hj0XcayHAvFlyLq7hyncXIpH1krEvZU2fMvvQVlN6xgMoaEYjbPy8KsTURtKWJuZQarskY4vbuGyaoK0dBLn2YXKg3mvLlUzl3E2oEjNLYPMzQZFBHiMomJ69DdRVPdIIHEVFbftIgCEeCupj6Kq0ckPta+5fgeXnjpOM1zPsaNN6xkeVmaskfUDDuzzfBk03ZeONBKfZ/7glcuiPkn8Q52sP+4HadLNPr7BxlXq8n62EYWZegwTdvoV6GxFlBaVMg1ljFa9zQTK83HVqLs7Dn1ktckWrCVVrDsxgrsR0/R3u9gbLp1rWfEmbrc239aNAJgMmEhd6/Ow3ylhlK9D11Fwx5iBHp388r2E+w5Nky6TYu75CY+vGku1fkp5+0n/06ixMJOOrY9yp6RZNyl1/LlddnxSnzm87lhWl7dw/Gj/Xhuvo87qlKx6tQz/z6Xg7hYosFBTvz2+/z7niQ01dfxZ5+7nuXi3F+W8VhRB1379nBsTyOjH/pz7i4zYvugN0tjUaKRMD5/DI0+iUR1grgoz/wsIAKhKiEBdaJIGkTQnX7ygkh2I06an32QHZ5CDJXr+eTyjMvTuAoPUPfMdmq7ArDlLu6otKAXFcBVUZal94BIPEJhImERbzXa+Lh3pWxHlJ6xcISYSpT1xKT4Mk3TxxPRGHb1Y294mZ8+n8Ka21exelk+1rccP/9uKeOt3QzsuJ8n+wowlS7nzrUFmGf8faS4yCht237H71+o43nRiP6Hf/gUq3MMWLRvMV/mUsQ8uAeb2POj3zBw/RdYvqSKhekXuLl5eAJXv/iM//UoLalGPLE8ikoWc92HV1JmVCaqTr3ufFNlXNkaW1kHO0GUcSWuTx+/A/jHe2h66te8YNjC0iWLuL4ideY7FJT12yN97P/5U7QZirFt2MwNpeZLXPP86nZVjfmNBSYYG59kYjJAovjWwgYbGcpOadp3k0xG8Ntbaa1vpnUEkpZcy2blNtdbzsq/WMpp9TJweC9NA35c6RXiwptDpl593nJL71ch/I5Oevc8zN8/OEzqqhu48eZ1XFuVMe3Ek0ul3I7pOPgcT+/u5pivmr/61l1UGRIwXBXn6r2klLMIvpEmTp9sptuXjGnhOlGWjUwzefhdUvpKvKIsvMrpMTWR7AoWzy+Jl2X59UiXU0zZTtY9QP2hI5z25JJbVsGaBRlvsYTTuxEj6nPg7drP0zUJWEqqmKusGGKRk9xmnogjMR8D+x7m/scbaAgVs/Fjd3DPygJSNAkzn4iFnIy2HeHA9u386GAeX/6rO9k0P/esbdzfgbKsZ9DLWP8IXpHphmI69HoTaTYzyjDwmfm4MfE2XnzDpzl6sBlHShm58xaxMufNU9/ePZGIex1073iJukgu1tJK5pVlkz6jm15dfa7a1R5mRMyPe3gQ+5hHJKdzqEzTvM2s/HcjwERXF45QIqTnUmTRXSUBNUpwrI3WQzv5zc92MjDvFm770HquWZQfT3hmTCxEyOfE0dtG3YHtbNt+hNpwKVnXf5Yff37hNMsbXZyop5++nj5OtwwTGBsmUrGB6iITZt8wXW29DMeSMUZGGXIZMWSXsnx5OfmiRX9VEpWKq78PuztKIL2YClGWZy4xVUJEkPHWVhwJJpLSs8hPkcmBdCWI5EAkIb7RDjocWnTmNIpyTTN6ZyO+cpCjg/oxS3xWfqZFf1UNd3g9zjWLODc+IuLc+nicM4k41z0V55JFnBsUcc4o4tyy9yTOxYj4x/F07uaBn75CLSWUbrqWe29cQFF8acKZIhLsqI+xvg5aT+5n/+797Kobx7H663zv80tZXpgygw2nmXLmMzs6OnCozCSl5VD8DksIXpxofPOisWaRXJvzsaRbyZitKzSdZXYnv9I0RJDyDNBxcAfbnz3AUx0FfPxvP861C3PJf9crCUQIBwL4PV4C4RD+gB+fslC+fYD+7laa6+s4umcXh9qMZGy4h8//3Zf58hLrJV6cfsbqj9DQ0k3tiJvExm1sV69n2aJSFmSIVvzEBBPqFMzhdk4e7WVMX8biO+/mE0ts8QkXsz0wSJJ0NRBxru4wDa09Z8W5DSxfVML8DEScc50V53oYM5Sz+I67+KSIc0o0vzJxTlluUcT7rlr2/fZBHmnKpurWG7n11uUstL3b9cGVoWGiLpn0iDolSEDULz6vm8nxUUYHOmhuqOfkwf0cqbXTr13JZ3/yb3xldVZ8gpckKWTyK51FuQXjxl6/lad/9TxP7nST87Wv8aml6aKleCmtc2X90rH4lqMTnkkc4w6Ge9poqTvJyVMNnB5W9nCChPRbuf0zX+DvvnUry1IusWci0s/xxw4yENJj21BKxp5/5sM/cpBcuZ577r2ZO69fQL6yrEysl0M/+x6PHJmkddmf89CfLcWsUslJXJIkvf+JOHfs0QMMhQ2kbyjBtvtf+PCPHZgqN3DPx2/izuvOjnP/xh+PeGhf/lUe+sqyN+6sxcKEg15cDieTnhhJKRbMqckkz9RKFxEfk30i1m99nH/+v7tJ+8pXuVl8rqW5xku4eyQ+c8CDo6cfp8fFmHOc0cFeelrqOH5INAYGPUwGoqiSqyla/gV++tCXWJOpI3U2D3KVziGTX+kNsQiR4d38/t9+xK8f3cne4QAJSUnxMcpvsV72BYsXM/GI/0f572iMaFTZ8lm04KdKYMoNX+ezX/oC37mzKh6YL4nrME+86CKgsXHNqhDtv/hLvvjKAq799Mf50j0rmZ86ddsn2sWBn/4rD+x1Urf4qzz7t+uwij/2skwYkyRJmkmuQzz+wiQhnY1NK4K0/eLrfHHbAq7/zMf54kdXiTg31WmhxLmfiDi3b4L6xX/GM3+7XsQ5ZS89ITDMcMs+/vij+3l6W5DCT36Ruz9/G7eWGZVnL5GI8+N1HHrqIf77X3/J4+1uoknKBN6Et9yE4cKdqUfO1ClK9RIV/1vpEVbqFOWnoK+6lsWf/g6P/X8rydCqZVyXXieH7UlTQkTDg5x44nc8f+AUJ+zKfuNhQgEfPp8Xr/fSHj6fOI7fj98fIBAIEgyFCMeDlPLeShCsZGllNSsqst/YZ/wSxNQ25i4uZ9GCVBKcvTTsbUNTNoc5c3LIP2uMWWy8j+42Fy63kfLiDMziGdnrK0nS1SCmzmDuknIWzk9F5eyLxzltWbmIc7kUmN6YCB4b76WrbQKXxxCPc8qupq/HuSQLqTnz+dAdS7CkVZKZZiMrbYZGxsbGaT/4CtteeJlXekWSriwpFxR1gKgPpqsnLu5xdp0SIBgMEQpHiEwlvpBHftYCblxVSmqiSLbjP5OkM2TPrzQlEl/up+/ILo4qi2A7A1M/vxKUEJ1B6fKlVM8rotA8A+3zsBt3UIQ7/yD2k1v5l3t/wNCXf8QXP7aRm6usJMbDYxTXiV/z/X/exolIJTd86y/42rI0uYKBJElXh3Pi3Isizv27iHM/FnFuw3lx7lcizm0Xca5KxLmv8RfL015PjJXnQxPdOPb8mHvvz2bzvTfx6bsWUDAjs/68jDadpKG+kZp+z9TPrpRUbIUVzF+3hHkzOgFY+iCQya/0ARYjPFJD7UuP8YXP7GP9L77PfbcuZFmWsoxMWMR8J7UP/D3//Mgk7rIb+Ktvf4RNqSH8iXqS1InM2Kp3kiRJl81UnNsq4txnRZz75Q+475YF58W5b/NPf3TjLd8i4tzdbExR4pxBxDm1iHMBPEMN1P36b/mrkXv42J2b+cSyFHAM0Gn3ElJnkZeXTqZVL4cNSB8YsnqXPsAi+CbsDPYN0GKoFAE8DatparZvNEDM286JvQ2MJKWSsaCMUnMIT1cLA04v7su9laokSdKMUOLc6OtxLl/EubRp4tyoRsS5+WWUmKbi3MRUnIv68LuGaTo8QEGBjdzkKKGBVppO7mb78w/xi0cOcbjFgVeGROkDRCa/0geYn8nxQfoGhvAuWkhhfgqWqf0zYwEPoY5j7DseRJdRwrKFeVjdg5x+9mka+pRtJmWklyTpaiDi3NgQvYMizi1eJOKcmdRz4txR9h57Lc7lYp0Uce6Zp6h/Lc6F3PhGRqg9YKYoRU9C9zFOnGqnRjOfW4smOC3+Xd8zjkuGROkDRCa/0gdXyIG9s5222gEylpWSbdJOTaaLEQ0FcA/30GVYTNn8apblh5i0d/JqcDl5lhQy9HKAmCRJV4GQndHOtjNxbmkpWWfFuUjIj3tIiXNLmLNgfjzOuRydbA+uIH8qzkUmx7D3drIzlMp4Sw29ESua8tXcvrySguX38tdfv5XbVxZgk9mC9AEii7P0wRXy4HGq8ftK2LL2TPJ7Zt8cFSqNAX2eSHrL9JjczdQdqWH/6RDl1ywhLy0Zg9zXX5Kkq0HIi1fEuYCvlC3rRJxLfiPOJShxLr+a5SLOJbubqBVx7sDpMBWblThnFHEugs/lYGSkm/68ZDwD9TTVN9PY5yKUZMZQsJz1y0opz05+H+6MJknvnpzwJr1ZeAJ7Xzftjb30DELe+lWU56WSNpNbG58v7Ga0q4PxkIaE7DLKUmdgaoW/j9aDDRw/OI7l47exOkeP+bWF25WxcOL5w88dY5AYAUMaJn02i9bNxSZeM1Pru0uSJF1WSpw7IOLcoXGsH7+dVTm6aeLcURHneD3OLVbinDaBJJWboWPPs+2xP/Cd8fV8qTJCbNLFuLGYnFVb+PxaGwFHAI1Bh86gvcjlwiL4nU4Cah2JOj3GS5lBHIueqSMGBxjq7aV3cDQ+N+NcCaiTxPuk55FfWERRXjrpKVfXVtXSlSOTX+lNYr5umvZu5clfvcDDz0X40APf45PXVlCdPsNbQ8Z3FvIxOT7B5Fgbx57bRpcql4zbP8snK/VTL5IkSZIui8ggjS8+zgsPb+fwh/6D796YgurgL3ho3yDbU+7m6b8spu2Yl8z8THILrBfR+xsR/+eifc9R7OZ8UotLqbC+y/ojGiLkdWJXtsJvb6f51GEO7t/Pq8daGdXmUGjVYxDJfiwSQ6XSkZw3l6qFK9lw7VpWLymnJFVz1rJuknSG7N+S3kSlL6B80XKuv2U1RvMCqgqtWE1nbqTNqPAEE31HePnXP+K7f/t3fO8/d7LzqHvqSUmSJOmy8o/j7Asw0pXFyjk2TFo9Gm0SxlQDKSlqDPZTHO1yMTARvbhe34gfPPXs+J8HeeGF45y2B6eeuFgxYoFR7M17uf+bv6cuWsWqP/0bvv3NP+Ubty6i4Mu/5IEX93L0+BEO7XmBp37xN3xthYeOh7/Hf/z3Izx0ZFBZ7E2S3kT9j8LUvyVpSgR3fz01tSd5wFPFJ+9eQFm6Ad1MN58TNGiMmeRVLmLl4nQiLZPEEm3kb17JgvTLkGxLkiRJr4uNNXGypo89Qxnc8Km1lJgSMUQncQyM0HKkjYF+yFuyiKpSGxbdGzvGvSNVIiSlU7h8JYtXzqc8y4Qu8ezfDuK1d9J+8EWeuP+nPPDCXrbv2svBvbvYtWuPeBxlUJON0awi0HaCw8/tpnnRHdy0rpIKW5Cx+lOceqkBx7I7uG1+Rnw+R6Iy5MEq6o+KbKItx2gOphHNqeLaxVkoKx7L3l/pbDL5labhZqTxMDXH6zlduIVPr8vB6m6jvf4Ye062UNOtxpaRjE6rvrRbB6oEEhK16E1mUjR2mne0MRZNJUcmv5IkSZefKgmtJZvCeRUsrMogJTEBjS4ZU1oepYVFlJaXU1meS0aKFs2FbJEW8zLR10rTvl0cOHmck5NWTOnp5Kbpzuo5DjDRcZL62kaOjujJn1NO8ZxKKueUUlZSTHGx8iiiqDifzBQ9+iRRR1hzKFo4n8psM6boCF2Hazi4bRDdzXeyeU4q6YZEUZ2oUSclivokwqB4/25tMRnzl7OlKi0+AVAmv9LZ5Jhf6c1CPdQ88QjPPXmMxrv/H39T4SbqG6O7q4nm1kFeHVjNP3xzE9WpHiYGBukYmFBGeF0AJfzoSJ9TRYHNRIr2tdRZFEH7Dn71tSepDxWy9J/+XI75lSRJuqrECI210N7cyvH6ftTukzzWvZqbPrSBOzcVY41nnzHC9joOHWqgcTQRU8USNi8uwqJVdpqLH+QdxexH2P6b5/if/+5k0a//g88tTSN/akv8WHiSsOMQD/zHs3RnrWbhlhu4oyyRie5GkWz70ReXUDg3nyy9HPE528kSIL1JzD3CUI+Lnm4dpcZJhpob6PCa0GeVsiBHi3PCizcSJegZZqCthgN79rDngh772Lv3CI0DLiYC0al3kyRJkq5+EXzD/YwHRIJbtYLVeYl4RyYYc/o4M+JX6WcLMdF0kFNDKrxZS7hldSkZ+gtPfJVjhCYcjEz66TEWUJJrjN+BjE+e9rtwDvfRdqKJ/owNLFi2kk1zTEQDYzjqX+T333+U515upMsn6x5J9vxKbxIj0Pw0D9y/nV887+L6+xaTt/xGrltYyByjD/e4g9O+bBYWGDEqQWdGyJ5fSZKkq5ufkaZ2nAEvfoObtu99nftzvsHHbt/Mx5ZloYlPPXNy/L9/SmvWCjJWbmZTzluvAKFSJYjH1P94XZCRQw/ym0eO87+NK3jwsU8wXyTPxpATe089J4+eYGuthRvuvY7lldnxxJqgk2j3s3zjLwfIvnkjt31+FRVaOQhitpPJr3SeAMP7fskvnzrAg8dj3FgYxp65mRtuvZbNiwrJFTmpP5qIXqPmQoaAXRiZ/EqSJF3dooQDQSKeXgbq9/Ifn34Qzd/8E3fevJS1+QZUhMRrRtj73b9na2+EQUMOeYbpOlCUbmA9FVs+yprqAkrOXvM9Zuf0E//FQ1tP8kRgMf/ffAeHt/fgsRRTtGIFK5fOZ35ZHlnpZpK1iSSKQ0W9I7iP/Zz7/pDJ6i0b+Nztc8mQ97xnPZn8SmcRRSE2Su1D/8FjpyZozL6Wry9VcXr/Tk7oVlO9eg33LknD1eshrchGkqeTtsZmjjUOx8PaO1OLlryRwtUbqS5MJ8v4WuCTya8kSdLVL0pg8AQ1rz7NF/7Nz5/+532sKMwgT6/DmmtGwwQn//cf2e7KIZy3lHV5yjoM51N6VZKwFleQK5LYVN1ZmarvNNt/+AteOG7HvukOPpYTxuMVNUuKjfS8fArys8m1GUlSqaYmuIUJOrvpfuY7/HXvTWxZv4DbS6C3oZeh0QA561ZTnJtOukb2BM82sv0jnUUkoUE7A21eYoFMlq1ey8qNm1ieMYFjfJj2wSEmxno4fHoMjz8iYpQadaIGrVZ7gQ8NGvFIUqtkwZMkSfrACeJ2DDLQ00tw5SqKLZO4HCO0DYs6JR71jWSVFmFKL8SSWcGSdetZv/78xzrxWMm8vNRzE19xhOBIO829MSYSytm4ejnzV93AzffcxYc/tJmNS+dQnJGM5vXEVxEmFJigt2kEmylKgnuM4c4e7GO9nHpmL03dYziDsv9vNpJLnUlviMWIOevZ/VQ3joRCFt20gqqUBDzdh6lxWdAmJpKndlPrzGRBaSpmiw1bXhnVCxey8IIeC1i4oJKidCNGZftNZZJCwM3YsIOJkUYOv3Ka/oCWlIVl5GvEc+ok1AkJ4iFb5ZIkSe9/PiZ6TtNeW0dP3mpWGUdwBvREkzMozzaK9FeNXuOlrW2cYbsfQ2YaqXotiSLGv3l879mUBDWI/fgzPHkizFj2cu75yHLK0w2iXkp4686UqBvPSAsHHniOPl0msaAOs8lEQaUNz0k7lqXzycmykCp7fmcd2QEnnUUkv2PD9OtshHOKKM3Uo0rQkTVvDXNCozj2H2fncT/rri3GnKy59MITnsA1WMvORx/liSde5VB/Kw2DNex7/gkefexFDnWM4fDLmbmSJElXBy3JtnwKKooxtW3nxV1uEvRpLJ6bNrXOr4pEkRRvrtZQEDzKg48epHF4EpcvQDAUInTWIxwV9dHrnbIRUT2N01nXwKTNSOaqcsqT3ybpfU3IS8DRQ1NdM9se2M6pLhe+nAqKKtfz2fv/L7etLaFQHEeafeSYX+ksoigEXIw4QkQTdaSmJ6NLiBEVPxuf8OMPgkavw5hqFj9XXfqEt1iESCiA2+UVgS6AZzJAKKZGk2zAkJSIxmhCrzkzaUGSJEl6vxP1RciH3+PC4UXEbi1Gk4jnBi1vbPAWJewbx9HfTnPNcY7uO0Rj0IpOp8McX4VBCfgGKm+6h7XVhZRaEkXu64bxA/zXt17FWb6aFR+6lhtKTe+Y/EadTXQc3sqffMfOrV9YgNXfzLA7Cf/Cj/IXZRMM6oqxppjJeH3+iTRbyORXkiRJkqQrKEbEP8mkvZ/uzl5GwzrUiWq06tcy5CQsheXkpE9thhQNQWCI+pMOVNZMMgszSFeWMXtbyuS7UzQffIG/OriEv/lsJVmjuzh0so3dvvnckp1C8YYVlOSlkyaHPcw6MvmVJEmSJOkDJkbI2cNw12lempzHlgXpWHwddHT1cGpYR5YujYqlpWSmJaOTue+sI5NfSZIkSZIkadaQoyklSZIkSZKkWUMmv5IkSZIkSdKsIZNfSZIkSZIkadaQya8kSZIkSZI0a8jkV5IkSZIkSZo1ZPIrSZIkSZIkzRoy+ZUkSZIkSZJmDZn8SpIkSZIkSbOGTH4lSZIkSZKkWUMmv5IkSZIkSdKsIZNfSZIkSZIkadaQya8kSZIkSZI0a8jkV5IkSZIkSZo1ZPIrSZIkSZIkzRoy+ZUkSZIkSZJmDZn8SpIkSZIkSbOGTH4lSZIkSZKkWUMmv5IkSZIkSdKsIZNfSZIkSZIkadaQya8kSZIkSZI0a8jkV5IkSZIkSZo1ZPIrSZIkSZIkzRoy+ZUkSZIkSZJmDZn8SpIkSZIkSbOGTH4lSZIkSZKkWUMmv5IkSZIkSdKsIZNfSZIkSZIkadaQya8kSZIkSZI0a8jkV5IkSZIkSZo1ZPIrSZIkSZIkzRoy+ZUkSZIkSZJmDZn8SpIkSZIkSbOGTH4lSZIkSZKkWUMmv5IkSZIkSdKsIZNfSZIkSZIkadaQya8kSZIkSZI0a8jkV5IkSZIkSZo1ZPIrSZIkSZIkzRoy+ZUkSZIkSZJmDZn8SpIkSZIkSbME/P8bCekTqSIU+wAAAABJRU5ErkJggg==)\n",
        "\n",
        "Don’t let the complex mathematical notations in the equation fool you, this is a pretty straight forward computation. On the right hand side, we are estimating the weight parameters for each client based on the loss values recorded across every data point they trained with. On the left, we scaled each of those parameters and sum them all component-wise.\n",
        "\n",
        "Below I have encapsulated this procedure into three simple functions."
      ],
      "metadata": {
        "id": "v278C7Ibo55C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_scalling_factor(clients_trn_data, client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "    #get the bs\n",
        "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
        "    #first calculate the total training data points across clinets\n",
        "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
        "    return local_count/global_count\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "        \n",
        "    return avg_grad\n",
        "\n",
        "\n",
        "'''\n",
        "I ran 100 global training loops as stipulated by the comms_round and on line 48 \n",
        "tested the trained global model after each communication round our test data. \n",
        "Here is the snippet for the test logic:\n",
        "'''\n",
        "\n",
        "def test_model(X_test, Y_test,  model, comm_round):\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    #logits = model.predict(X_test, batch_size=100)\n",
        "    logits = model.predict(X_test)\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
        "    return acc, loss"
      ],
      "metadata": {
        "id": "8CJKtoRwfz-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating train-test split**\n",
        "\n",
        "> A couple of steps took place in this snippet. We applied the load function defined in the previous code block to obtain the list of images (now in numpy arrays) and label lists. After that, we used the LabelBinarizer object from sklearn to 1-hot-encode the labels. Going forward, rather than having the label for digit 1 as number 1, it will now have the form[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]. With this labelling style, we’ll be able to use the cross-entropy loss in Tensorflow as our model’s loss function. Alternatively, I could have left the labels as it was and use the sparse-categorical-entropy loss instead. Finally, I used the sklearn’s train_test_split object to split the data into a train/test with ratio 9:1\n",
        "\n"
      ],
      "metadata": {
        "id": "NGVEYDkMXv1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#declear path to your mnist data folder\n",
        "img_path = '/content/trainingSet/'\n",
        "\n",
        "#get the path list using the path object\n",
        "image_paths = list(paths.list_images(img_path))\n",
        "\n",
        "#apply our function\n",
        "image_list, label_list = load(image_paths, verbose=10000)\n",
        "\n",
        "#binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "label_list = lb.fit_transform(label_list)\n",
        "\n",
        "#split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_list, \n",
        "                                                    label_list, \n",
        "                                                    test_size=0.1, \n",
        "                                                    random_state=42)\n",
        "\n",
        "#create clients\n",
        "clients = create_clients(X_train, y_train, num_clients=10, initial='client')\n",
        "\n",
        "'''\n",
        "While applying this function batch_data, I will process the test set as well and keep it aside for later use.\n",
        "'''\n",
        "#process and batch the training data for each client\n",
        "clients_batched = dict()\n",
        "for (client_name, data) in clients.items():\n",
        "    clients_batched[client_name] = batch_data(data)\n",
        "    \n",
        "#process and batch the test set  \n",
        "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
        "\n",
        "'''\n",
        "To build a new model, the build method will be invoked. \n",
        "It requires the input data’s shape and the number of classes as arguments. \n",
        "With MNIST, the shape parameter will be 28*28*1 = 784,while the number of classes will be 10.\n",
        "Now is the time to define an optimizer, loss function and metrics to compile our models with later on.\n",
        "'''\n",
        "comms_round = 100\n",
        "    \n",
        "#create optimizer\n",
        "lr = 0.01 \n",
        "loss='categorical_crossentropy'\n",
        "metrics = ['accuracy'] # f1_score, precision_score\n",
        "optimizer = SGD(lr=lr, \n",
        "                decay=lr / comms_round, \n",
        "                momentum=0.9\n",
        "               ) \n",
        "\n",
        "'''\n",
        "SGD is my default optimizer except when I have a reason not to use it. \n",
        "The loss function is categorical_crossentropy. \n",
        "And finally, the metric I will be using is accuracy. \n",
        "But something looks strange in the decay argument. \n",
        "What’s comms_round? \n",
        "It’s simply the number global epochs (aggregations) I will be running during training. \n",
        "So rather than decaying the learning rate with respect to the number of local epochs as you might be familiar with, \n",
        "here I want to decay with respect to the number of global aggregation. \n",
        "This is obviously an hyper parameter selection choice, \n",
        "but I found it to work pretty well while experimenting.\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "nLDIVlc5X1zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Federated Model Training**\n",
        "\n",
        "The training logic has two main loops, the outer loop is for the global iteration, the inner is for iterating through client’s local training. There’s an implicit third one though, it accounts for the local epochs and will be taken care of by the epochs argument in our model.fit method.\n",
        "\n",
        "Starting out I built the global model with input shape of (784,) and number of classes as 10 — lines 2–3. I then stepped into the outer loop. First obtaining the initialised weights of the global model on line 9. Lines 15 and 16 shuffles the clients dictionary order to ensure randomness. From there, I started iterating through client training.\n",
        "\n",
        "For each client, I created a new model object, compiled it and set it’s initialisation weights to the current parameters of the global model — lines 20–27. The local model (client) was then trained for one epoch. After training, the new weights were scaled and appended to the scaled_local_weight_list on line 35. That was it for local training.\n",
        "\n",
        "Moving back into the outer loop on line 41, I summed up all the scaled local trained weights (of course by components) and updated the global model to this new aggregate. That ends a full global training epoch."
      ],
      "metadata": {
        "id": "emrKxCpivQ7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize global model\n",
        "smlp_global = SimpleMLP()\n",
        "global_model = smlp_global.build(784, 10)\n",
        "        \n",
        "#commence global training loop\n",
        "for comm_round in range(comms_round):\n",
        "            \n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "    \n",
        "    #initial list to collect local model weights after scalling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    #randomize client data - using keys\n",
        "    client_names= list(clients_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "        \n",
        "    #loop through each client and create new local model\n",
        "    for client in client_names:\n",
        "        smlp_local = SimpleMLP()\n",
        "        local_model = smlp_local.build(784, 10)\n",
        "        local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "        \n",
        "        #set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "        #fit local model with client's data\n",
        "        tf.autograph.experimental.do_not_convert(local_model.fit(clients_batched[client], epochs=1, verbose=0))\n",
        "        \n",
        "        #scale the model weights and add to list\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "        \n",
        "        #clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "        \n",
        "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    #update global model \n",
        "    global_model.set_weights(average_weights)\n",
        "\n",
        "    #test global model and print out metrics after each communications round\n",
        "    for(X_test, Y_test) in test_batched:\n",
        "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
        "    \n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7ayWp8XvR9s",
        "outputId": "362c65c1-73bf-4892-91a9-beb6ce72a671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comm_round: 0 | global_acc: 88.476% | global_loss: 1.6665740013122559\n",
            "comm_round: 1 | global_acc: 90.738% | global_loss: 1.6157482862472534\n",
            "comm_round: 2 | global_acc: 91.524% | global_loss: 1.5961037874221802\n",
            "comm_round: 3 | global_acc: 92.214% | global_loss: 1.5862112045288086\n",
            "comm_round: 4 | global_acc: 92.452% | global_loss: 1.5777422189712524\n",
            "comm_round: 5 | global_acc: 93.071% | global_loss: 1.570860743522644\n",
            "comm_round: 6 | global_acc: 93.333% | global_loss: 1.5650556087493896\n",
            "comm_round: 7 | global_acc: 93.881% | global_loss: 1.5594133138656616\n",
            "comm_round: 8 | global_acc: 94.095% | global_loss: 1.5569168329238892\n",
            "comm_round: 9 | global_acc: 94.381% | global_loss: 1.5527793169021606\n",
            "comm_round: 10 | global_acc: 94.667% | global_loss: 1.5495015382766724\n",
            "comm_round: 11 | global_acc: 94.929% | global_loss: 1.547296166419983\n",
            "comm_round: 12 | global_acc: 94.976% | global_loss: 1.5444680452346802\n",
            "comm_round: 13 | global_acc: 94.976% | global_loss: 1.5427590608596802\n",
            "comm_round: 14 | global_acc: 95.000% | global_loss: 1.5411375761032104\n",
            "comm_round: 15 | global_acc: 95.190% | global_loss: 1.5395283699035645\n",
            "comm_round: 16 | global_acc: 95.381% | global_loss: 1.5380533933639526\n",
            "comm_round: 17 | global_acc: 95.500% | global_loss: 1.5362051725387573\n",
            "comm_round: 18 | global_acc: 95.571% | global_loss: 1.5353578329086304\n",
            "comm_round: 19 | global_acc: 95.429% | global_loss: 1.5345008373260498\n",
            "comm_round: 20 | global_acc: 95.643% | global_loss: 1.5334742069244385\n",
            "comm_round: 21 | global_acc: 95.690% | global_loss: 1.532031774520874\n",
            "comm_round: 22 | global_acc: 95.762% | global_loss: 1.5308302640914917\n",
            "comm_round: 23 | global_acc: 95.905% | global_loss: 1.5304025411605835\n",
            "comm_round: 24 | global_acc: 95.667% | global_loss: 1.5294970273971558\n",
            "comm_round: 25 | global_acc: 95.905% | global_loss: 1.5288139581680298\n",
            "comm_round: 26 | global_acc: 95.976% | global_loss: 1.5282857418060303\n",
            "comm_round: 27 | global_acc: 95.857% | global_loss: 1.5270644426345825\n",
            "comm_round: 28 | global_acc: 96.024% | global_loss: 1.5265859365463257\n",
            "comm_round: 29 | global_acc: 96.000% | global_loss: 1.5260405540466309\n",
            "comm_round: 30 | global_acc: 96.000% | global_loss: 1.5256295204162598\n",
            "comm_round: 31 | global_acc: 96.119% | global_loss: 1.5252376794815063\n",
            "comm_round: 32 | global_acc: 96.167% | global_loss: 1.5242631435394287\n",
            "comm_round: 33 | global_acc: 96.095% | global_loss: 1.523974895477295\n",
            "comm_round: 34 | global_acc: 96.167% | global_loss: 1.52354097366333\n",
            "comm_round: 35 | global_acc: 96.214% | global_loss: 1.5229839086532593\n",
            "comm_round: 36 | global_acc: 96.214% | global_loss: 1.5224095582962036\n",
            "comm_round: 37 | global_acc: 96.286% | global_loss: 1.5218836069107056\n",
            "comm_round: 38 | global_acc: 96.286% | global_loss: 1.5213258266448975\n",
            "comm_round: 39 | global_acc: 96.333% | global_loss: 1.5213062763214111\n",
            "comm_round: 40 | global_acc: 96.333% | global_loss: 1.521000623703003\n",
            "comm_round: 41 | global_acc: 96.357% | global_loss: 1.5205479860305786\n",
            "comm_round: 42 | global_acc: 96.405% | global_loss: 1.5202099084854126\n",
            "comm_round: 43 | global_acc: 96.310% | global_loss: 1.5200059413909912\n",
            "comm_round: 44 | global_acc: 96.357% | global_loss: 1.5197144746780396\n",
            "comm_round: 45 | global_acc: 96.452% | global_loss: 1.5192160606384277\n",
            "comm_round: 46 | global_acc: 96.429% | global_loss: 1.5190792083740234\n",
            "comm_round: 47 | global_acc: 96.476% | global_loss: 1.5186598300933838\n",
            "comm_round: 48 | global_acc: 96.500% | global_loss: 1.5183680057525635\n",
            "comm_round: 49 | global_acc: 96.429% | global_loss: 1.5181382894515991\n",
            "comm_round: 50 | global_acc: 96.452% | global_loss: 1.5178982019424438\n",
            "comm_round: 51 | global_acc: 96.571% | global_loss: 1.5179109573364258\n",
            "comm_round: 52 | global_acc: 96.476% | global_loss: 1.517454743385315\n",
            "comm_round: 53 | global_acc: 96.548% | global_loss: 1.5171841382980347\n",
            "comm_round: 54 | global_acc: 96.548% | global_loss: 1.5168352127075195\n",
            "comm_round: 55 | global_acc: 96.476% | global_loss: 1.516735315322876\n",
            "comm_round: 56 | global_acc: 96.500% | global_loss: 1.5164498090744019\n",
            "comm_round: 57 | global_acc: 96.595% | global_loss: 1.516302227973938\n",
            "comm_round: 58 | global_acc: 96.714% | global_loss: 1.5159693956375122\n",
            "comm_round: 59 | global_acc: 96.619% | global_loss: 1.5157766342163086\n",
            "comm_round: 60 | global_acc: 96.643% | global_loss: 1.5157089233398438\n",
            "comm_round: 61 | global_acc: 96.667% | global_loss: 1.5153528451919556\n",
            "comm_round: 62 | global_acc: 96.667% | global_loss: 1.5153456926345825\n",
            "comm_round: 63 | global_acc: 96.619% | global_loss: 1.514939546585083\n",
            "comm_round: 64 | global_acc: 96.643% | global_loss: 1.5147775411605835\n",
            "comm_round: 65 | global_acc: 96.619% | global_loss: 1.5145598649978638\n",
            "comm_round: 66 | global_acc: 96.714% | global_loss: 1.514479160308838\n",
            "comm_round: 67 | global_acc: 96.738% | global_loss: 1.514372706413269\n",
            "comm_round: 68 | global_acc: 96.690% | global_loss: 1.513949990272522\n",
            "comm_round: 69 | global_acc: 96.714% | global_loss: 1.5138225555419922\n",
            "comm_round: 70 | global_acc: 96.690% | global_loss: 1.5139060020446777\n",
            "comm_round: 71 | global_acc: 96.690% | global_loss: 1.5136160850524902\n",
            "comm_round: 72 | global_acc: 96.619% | global_loss: 1.5134971141815186\n",
            "comm_round: 73 | global_acc: 96.714% | global_loss: 1.5133203268051147\n",
            "comm_round: 74 | global_acc: 96.714% | global_loss: 1.5132626295089722\n",
            "comm_round: 75 | global_acc: 96.714% | global_loss: 1.5130417346954346\n",
            "comm_round: 76 | global_acc: 96.714% | global_loss: 1.5128456354141235\n",
            "comm_round: 77 | global_acc: 96.690% | global_loss: 1.5127490758895874\n",
            "comm_round: 78 | global_acc: 96.786% | global_loss: 1.5126529932022095\n",
            "comm_round: 79 | global_acc: 96.667% | global_loss: 1.5126255750656128\n",
            "comm_round: 80 | global_acc: 96.690% | global_loss: 1.5124974250793457\n",
            "comm_round: 81 | global_acc: 96.762% | global_loss: 1.5120768547058105\n",
            "comm_round: 82 | global_acc: 96.762% | global_loss: 1.512178659439087\n",
            "comm_round: 83 | global_acc: 96.786% | global_loss: 1.5118530988693237\n",
            "comm_round: 84 | global_acc: 96.738% | global_loss: 1.5117173194885254\n",
            "comm_round: 85 | global_acc: 96.738% | global_loss: 1.5117086172103882\n",
            "comm_round: 86 | global_acc: 96.762% | global_loss: 1.5116994380950928\n",
            "comm_round: 87 | global_acc: 96.833% | global_loss: 1.5115234851837158\n",
            "comm_round: 88 | global_acc: 96.762% | global_loss: 1.5114020109176636\n",
            "comm_round: 89 | global_acc: 96.738% | global_loss: 1.5111671686172485\n",
            "comm_round: 90 | global_acc: 96.738% | global_loss: 1.51111900806427\n",
            "comm_round: 91 | global_acc: 96.690% | global_loss: 1.5111855268478394\n",
            "comm_round: 92 | global_acc: 96.714% | global_loss: 1.5108896493911743\n",
            "comm_round: 93 | global_acc: 96.786% | global_loss: 1.5108505487442017\n",
            "comm_round: 94 | global_acc: 96.786% | global_loss: 1.5107921361923218\n",
            "comm_round: 95 | global_acc: 96.762% | global_loss: 1.5104963779449463\n",
            "comm_round: 96 | global_acc: 96.810% | global_loss: 1.5105260610580444\n",
            "comm_round: 97 | global_acc: 96.762% | global_loss: 1.510507345199585\n",
            "comm_round: 98 | global_acc: 96.762% | global_loss: 1.5104442834854126\n",
            "comm_round: 99 | global_acc: 96.738% | global_loss: 1.5102542638778687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results\n",
        "With 10 clients each running 1 local epoch on top of 100 global communication rounds, here is the truncated test result:"
      ],
      "metadata": {
        "id": "WQapD4Nqv4n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SGD Vs Federated Averaging**\n",
        "\n",
        "Yes, our FL model results are great, 96.5% test accuracy after 100 communication rounds. But how does it compare to a standard SGD model trained on the same data set? To find out, I’ll train a single 3-layer MLP model (rather 10 as we did in FL) on the combined training data. Remember the combined data was our training data prior to partitioning.\n",
        "\n",
        "To ensure an equal playing ground, I will retain every hyper parameter used for the FL training except the batch size. Rather than using 32 , our SGD’s batch size will be 320. With this setting, we are sure that the SGD model would see exactly the same number of training samples per epoch as the global model did per communication round in FL.\n",
        "\n"
      ],
      "metadata": {
        "id": "T1m3-3ckwMmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)\n",
        "smlp_SGD = SimpleMLP()\n",
        "SGD_model = smlp_SGD.build(784, 10) \n",
        "\n",
        "SGD_model.compile(loss=loss, \n",
        "              optimizer=optimizer, \n",
        "              metrics=metrics)\n",
        "\n",
        "# fit the SGD training data to model\n",
        "_ = SGD_model.fit(SGD_dataset, epochs=100, verbose=0)\n",
        "\n",
        "#test the SGD global model and print out metrics\n",
        "for(X_test, Y_test) in test_batched:\n",
        "        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OkXN_TowIgF",
        "outputId": "dfa13799-fb07-4f17-b9d3-90e2eaf7bb57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comm_round: 1 | global_acc: 94.857% | global_loss: 1.5432435274124146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There you have it, a 94.5% test accuracy for the SGD model after 100 epochs. Isn’t it surprising that the FL performed a little better than its SGD counterpart with this data set? I warn you not to get too excited about this though. These kind of results are not likely in real world scenario. Yeah! Real world federated data held by clients are mostly NON independent and identically distributed (IID)."
      ],
      "metadata": {
        "id": "ZXJnKgZvwW7m"
      }
    }
  ]
}